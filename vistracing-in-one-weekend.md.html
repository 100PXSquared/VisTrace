                                   **VisTracing in One Weekend**
                      Inspired by [Ray Tracing in One Weekend](https://raytracing.github.io/books/RayTracingInOneWeekend.html) by [Peter Shirley](https://github.com/petershirley)

Introduction
============
Many people have written ray tracers in Garry's Mod, but few have explored the more serious side of light transport, reserving themselves to basic lighting models and guesswork. With VisTracing in One Weekend I hope to provide a gateway into the wider field of light transport, while remaining in GMod and assuming only a minimal understanding of ray tracing.  

If you're entirely unfamiliar with ray tracing, specifically ray tracing in GMod, I would recommend you read Peter Shirley's Ray Tracing in One Weekend which is linked above, and is the inspiration for this book.  

By the end of each section you should have a solid groundwork to either move into the next section with, or to develop onto yourself. And by the end of the book you'll have a pretty decent tracer.

While you could technically follow along without these, the book assumes you'll be using them:
* Garry's Mod
* VisTrace (subscribe to the Workshop addon and download the latest release binary from the GitHub)

You should also already be familiar with Lua (specifically GLua) and basic rendering terminology like shading.  

You'll be implementing the tracer as a simple GLua script run with `lua_openscript_cl` from the console, however you could follow along from Starfall or even write a fully fledged addon around the renderer.  

Drawing an Image
----------------

Before we get into the nitty gritty of light transport, we need to actually be able to draw pixels to our screen. While there are *many* ways to do this ranging from drawing to a physical screen in the world, to a VGUI frame you can use like a mini application, I've opted for the simplest and most efficient for this step (however you can ignore this and use whatever method you prefer if you're already familiar with rendering in GMod).  

To start with we need a script to work in, and an addon to place that script in. Create a new folder in your `garrysmod/addons` folder with any name you like, then create a subfolder called `lua` and a file in that folder called `vistracer.lua` (you can of course use whatever name you want, however the book assumes this is what you called it). You should now have the file `garrysmod/addons/youraddonname/lua/vistracer.lua`.  

Open the file in your text editor of choice (I recommend vscode with sumneko's Lua extension and the GLua annotations) and add the following
~~~ lua
print("VisTracing in One Weekend!")
~~~

![Console Output](./images/vistracing-in-one-weekend/console-print.png)

Launch GMod and start singleplayer (or join a multiplayer server with clientside Lua enabled), open console with the grave/backtick key, and enter `lua_openscript_cl vistracer.lua`. You should see `VisTracing in One Weekend!` appear in your console! If not then check that `sv_allowcslua` is set to `1`, and that GMod has been restarted since you created `vistracer.lua` (it only hotloads existing files).  

Now to draw something to the screen we need a render context, and in this case I've chosen the `HUDPaint` hook as a simple way to get pixels above everything except the menu. Remove the print we added and replace it with the hook
~~~ lua
hook.Add("HUDPaint", "VisTracer", function()

end)
~~~  

This wont do anything yet, but go ahead and run it just to check everything's working. We have a render context, but we need to actually draw something. For now lets just use a red rectangle
~~~ lua
hook.Add("HUDPaint", "VisTracer", function()
	~~~ lua highlight
	surface.SetDrawColor(255, 0, 0)
	surface.DrawRect(0, 0, 256, 256)
	~~~ lua
end)
~~~

![`surface.DrawRect`](./images/vistracing-in-one-weekend/surface-draw.png)

You may be thinking that this is how we'll render an entire image, however if you were to add a print to this hook you'd see that it's called every frame. You can imagine how laggy our game would get if we were ray tracing a scene every frame in Lua.

Instead we need a way to render a set of pixels only once, and save them for future frames. To do this we're going to use *render targets*, which are a special kind of texture we're able to draw to, and the pixels we draw will be saved indefinitely (quite literally in fact, GMod doesn't destroy render targets until the game closes).  

First of all we need to create a render target to use, in addition to a material to assign the render target to as a base texture in order to draw it to the screen later
~~~ lua
local rt = GetRenderTargetEx(
	"VisTracer",                     -- Name of the render target
	1, 1, RT_SIZE_FULL_FRAME_BUFFER, -- Resize to screen res automatically
	MATERIAL_RT_DEPTH_SEPARATE,      -- Create a dedicated depth/stencil buffer
	bit.bor(1, 256),                 -- Texture flags for point sampling and no mips
	0,                               -- No RT flags
	IMAGE_FORMAT_RGBA8888            -- RGB image format with 8 bits per channel
)

local rtMat = CreateMaterial("VisTracer", "UnlitGeneric", {
	["$basetexture"] = rt:GetName(),
	["$translucent"] = "1" -- Enables transparency on the material
})

...
~~~

Next we draw the render target to the screen
~~~ lua
...

hook.Add("HUDPaint", "VisTracer", function()
	~~~ lua delete
	surface.SetDrawColor(255, 0, 0)
	surface.DrawRect(0, 0, 256, 256)
	~~~ lua highlight
	render.SetMaterial(rtMat)
	render.DrawScreenQuad() -- Draws a quad to the entire screen
	~~~ lua
end)
~~~

If you run this with `lua_openscript_cl vistracer.lua` you should get a black screen (don't worry, use `lua_run_cl hook.Remove("HUDPaint", "VisTracer")` to get rid of the hook). This isn't exactly what we want, so lets try drawing some pixels.  

While we could draw to the render target immediately when we run our script, we're going to want to split the drawing over multiple frames for later (ray tracing single threaded in Lua is not fast). To do this we can ignore more complicated (and slower) methods of spreading our code over multiple frames, and just render one row per frame. We also need to define an x and y resolution to draw, which we'll place at the top of the file (this is where all of our parameters will go)
~~~ lua highlight
local RESX, RESY = 256, 256
~~~ lua

...


~~~ lua highlight
local y = 0
~~~ lua
hook.Add("HUDPaint", "VisTracer", function()
	~~~ lua highlight
	if y < RESY then
		render.PushRenderTarget(rt)

		for x = 0, RESX - 1 do
			render.SetViewPort(x, y, 1, 1)
			render.Clear(255, 0, 0, 255, true, true)
		end

		render.PopRenderTarget()
		y = y + 1
	end
	~~~ lua

	render.SetMaterial(rtMat)
	render.DrawScreenQuad()
end)
~~~

Running this should produce a 256x256 red rectangle just like the original `surface.DrawRect` method, except we now have control over the colour of individual pixels and we're only drawing each pixel once. You may also notice that we still have the same problem as before where the entire screen is black except for where we drew our rectangle.

To fix that we'll add a `setup` flag and clear the render target with 0 alpha before we start writing.
~~~ lua
...

local y = 0

	~~~ lua highlight
local setup = true
	~~~ lua
hook.Add("HUDPaint", "VisTracer", function()
	if y < RESY then
		render.PushRenderTarget(rt)

		~~~ lua highlight
		if setup then
			render.Clear(0, 0, 0, 0, true, true)
			setup = false
		end
		~~~ lua

		for x = 0, RESX - 1 do
			render.SetViewPort(x, y, 1, 1)
			render.Clear(255, 0, 0, 255, true, true)
		end

		render.PopRenderTarget()
		y = y + 1
	end

	render.SetMaterial(rtMat)
	render.DrawScreenQuad()
end)
~~~

Et voila! You should now have a significantly slower to draw red rectangle! Not impressed? Well lets draw something slightly more interesting that we couldn't do with a simple `surface.DrawRect` call...
~~~ lua
		...

		for x = 0, RESX - 1
			render.SetViewPort(x, y, 1, 1)

			~~~ lua delete
			render.Clear(255, 0, 0, 255, true, true)
			~~~ lua

			~~~ lua highlight
			render.Clear(x / (RESX - 1) * 255, y / (RESY - 1) * 255, 0, 255, true, true)
			~~~ lua
		end

		...
~~~

Running our script now should produce a nice gradient from black to red on the x axis, and black to green on the y axis. You may also notice that the pixels are black in the *top left* of the rectangle, and not the bottom left as you might expect.

![Red on the X axis and green on the Y axis](./images/vistracing-in-one-weekend/xy-square.png)

Tracing a Ray
-------------

Before we dig into the meat of ray tracing, we need to know how to trace a ray, and to know that we need to know what a "ray" even is.

While it makes sense to think of a ray as a single "ray" of light (light is far more complicated than that in reality), it's actually the total of *all* light that follows a specific line defined by our ray.

Additionally we can define a ray as $\mathbf{x} = \mathbf{o} + t\mathbf{d}$, where $\mathbf{x}$ is the hit point of the ray, $\mathbf{o}$ is the origin, $\mathbf{d}$ is the direction, and $t$ is the *time* or distance along the ray.

Tracing a ray in this context is solving the above equation for the lowest positive value of $t$, which we do by testing every part of the scene geometry for intersections with the ray. We wont have to worry about tracing the ray ourselves, as VisTrace handles this for us, but it's good to know roughly what's going on under the hood.

To trace a ray with VisTrace, it needs to know the scene geometry we want to trace before we can actually trace with it, which we can do by building an *acceleration structure*. The specifics of ray tracing acceleration structures are well outside the scope of this book, but if you want to look into them then VisTrace uses a [bounding volume hierarchy](https://en.wikipedia.org/wiki/Bounding_volume_hierarchy).

Thankfully building an acceleration structure with modern versions of VisTrace is easy, we simply call a function passing in an array of entities we want to intersect with, along with a boolean to toggle intersections with the world (by default it's true). For now we'll just trace any entities whose class starts with `prop_` and disable world tracing. I've also added some comments to help keep our code clean

~~~ lua highlight
----------------
-- Parameters --
----------------
~~~ lua
local RESX, RESY = 256, 256


~~~ lua highlight
----------------
--    Init    --
----------------
local accel = vistrace.CreateAccel(ents.FindByClass("prop_*"), false)
~~~ lua

local rt = GetRenderTargetEx(
	"VisTracer",                     -- Name of the render target
	1, 1, RT_SIZE_FULL_FRAME_BUFFER, -- Resize to screen res automatically
	MATERIAL_RT_DEPTH_SEPARATE,      -- Create a dedicated depth/stencil buffer
	bit.bor(1, 256),                 -- Texture flags for point sampling and no mips
	0,                               -- No RT flags
	IMAGE_FORMAT_RGBA8888            -- RGB image format with 8 bits per channel
)

...
~~~

Now we have our scene geometry in an acceleration structure we can start tracing some rays. While we could put all of this in the render hook from the previous section, to help with readability we're going to create a new function above our hook called `TracePixel`, which will be called from the hook with the x and y coordinates of the current pixel and will return an RGB vector
~~~ lua
...


~~~ lua highlight
local function TracePixel(x, y)
	return Vector(x / (RESX - 1), y / (RESY - 1), 0)
end
~~~ lua

local y = 0
local setup = true
hook.Add("HUDPaint", "VisTracer", function()
	if y < RESY then
		render.PushRenderTarget(rt)
		if setup then
			render.Clear(0, 0, 0, 0, true, true)
			setup = false
		end

		for x = 0, RESX - 1 do
			~~~ lua highlight
			local rgb = TracePixel(x, y)
			~~~ lua

			render.SetViewPort(x, y, 1, 1)
			~~~ lua delete
			render.Clear(x / (RESX - 1) * 255, y / (RESY - 1) * 255, 0, 255, true, true)
			~~~ lua highlight
			render.Clear(rgb[1] * 255, rgb[2] * 255, rgb[3] * 255, 255, true, true)
			~~~ lua
		end

		render.PopRenderTarget()
		y = y + 1
	end

	render.SetMaterial(rtMat)
	render.DrawScreenQuad() -- Draws a quad to the entire screen
end)
~~~

This should produce the exact same image as the end of the last section, however now we can add as much code as we want to `TracePixel` without cluttering the render hook. Also if you're wondering why we're doing `* 255` on the final RGB vector, instead of using 0-255 colours in `TracePixel`, it's because when doing any kind of rendering having `grey * grey` produce 255 (after clamping) is really not helpful.

To trace our ray, we need to know the start position and the direction to trace in. We'll get into generating camera rays in a bit but for now we're going to do [orthographic projection](https://en.wikipedia.org/wiki/Orthographic_projection), meaning all of our rays point in the exact same direction and the origin changes. This has the effect of no matter how close or far away something is, it will always look the same in our final image.

We're also going to use the player camera's origin and angles for our rays, although you could use anything you want
~~~ lua highlight
local camPos, camAng = LocalPlayer():EyePos(), LocalPlayer():EyeAngles()
~~~ lua
local function TracePixel(x, y)
	~~~ lua highlight
	local origin = camPos + camAng:Right() * x / RESX * 100 + camAng:Up() * (1 - y / RESY) * 100
	local camDir = camAng:Forward()

	local result = accel:Traverse(origin, camDir)
	if result then
		return Vector(1, 1, 1)
	else
		return Vector(0, 0, 0)
	end
	~~~ lua delete
	return Vector(x / (RESX - 1), y / (RESY - 1), 0)
	~~~ lua
end
~~~

![Kleiner sitting on a crate](./images/vistracing-in-one-weekend/orthographic.png)

Because we're just adding onto the player camera's position the rays aren't centred, so you'll need to look down and to the left, but we're now tracing a scene!

Ray Tracing
===========

We have pixels being drawn to the screen, and we have rays being traced, now it's time to simulate light with ray tracing. I wont bore you with a bunch of theory right away, but I will give you an overview of how the ray tracing algorithm works.

We start by generating a ray from a pixel given our camera's properties as well as any additional properties that affect ray generation (like jittering for MSAA), we then trace this ray out into the scene to get a hit point. If we missed it's up to us how we want to colour that pixel, however if we hit an object we plug the information about the hit as well as our ray into the *rendering equation*, which will determine the colour of the hit point that we then assign to the pixel.

We'll be simulating three kinds of light interactions in our ray tracer:
* Perfect diffuse reflection - Light is scattered in roughly all directions from the surface, i.e. Lambertian reflection.
* Perfect specular reflection - Light is scattered entirely in the direction obtained by reflecting our view direction about the surface normal
* Perfect specular transmission - Light is scattered entirely in the direction obtained by refracting our view direction about the surface normal

Generating Camera Rays
----------------------

Before we start lighting our scene, we need to replace the orthographic projection we did earlier with a perspective camera, specifically a [pinhole camera](https://en.wikipedia.org/wiki/Pinhole_camera).

We could parameterise our camera with a field of view value, however I've chosen focal length and sensor height as this can be integrated into various physically based post processing steps and ray generation upgrades like depth of field.

Add these to our parameters section at the top of the file. I've chosen a full frame sensor height of 35mm and a focal length of 60mm fairly arbitrarily, so feel free to play around with them
~~~ lua
----------------
-- Parameters --
----------------
local RESX, RESY = 256, 256
~~~ lua highlight
local FOCAL_LENGTH = 60
local SENSOR_HEIGHT = 35
~~~

Next we'll cache a couple of values that we'll use when generating a ray that wont change over a trace. These values are derived from using a field of view to determine the vertical scale, and then simplifying the equation using focal length to FoV conversion equations.
~~~ lua
----------------
--    Init    --
----------------
local accel = vistrace.CreateAccel(ents.FindByClass("prop_*"), false)


~~~ lua highlight
local camScaleVertical = 0.5 * SENSOR_HEIGHT / FOCAL_LENGTH
local camScaleHorizontal = RESX / RESY * camScaleVertical
~~~

If you want a more detailed explanation of these or the FoV equivalents then take a look at [Scratchapixel's raygen tutorial](https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-generating-camera-rays/generating-camera-rays).

Now lets change our old orthographic projection code to the pinhole camera model using our vertical and horizontal scale values. Before we can scale our values though we need to centre our pixel coordinate to the camera's origin, as well as converting it to a unit square coordinate.

To start with we'll offset our pixel by 0.5 so that our coordinate represents the centre of the pixel and not the corner, and then normalise it by dividing by the width and height of the image respectively
\begin{equation}
cam_x = \frac{pixel_x + 0.5}{res_x} \\
cam_y = \frac{pixel_y + 0.5}{res_y}
\end{equation}

Then we'll convert this 0-1 coordinate to the unit square coordinates of -1 to 1. Note that whether you subtract the normalised pixel from 1, or 1 from the pixel, depends on the handedness of your coordinate system
\begin{equation}
cam_x = 1 - 2 * \frac{pixel_x + 0.5}{res_x} \\
cam_y = 1 - 2 * \frac{pixel_y + 0.5}{res_y}
\end{equation}

Finally we'll apply our horizontal and vertical scale values
\begin{equation}
cam_x = (1 - 2 * \frac{pixel_x + 0.5}{res_x}) * scale_h \\
cam_y = (1 - 2 * \frac{pixel_y + 0.5}{res_y}) * scale_v
\end{equation}

This gives us a position in camera space, which we'll then normalise and rotate to give us a direction in world space.

Finally, lets add this in code
~~~ lua
local camPos, camAng = LocalPlayer():EyePos(), LocalPlayer():EyeAngles()
local function TracePixel(x, y)
	~~~ lua delete
	local origin = camPos + camAng:Right() * x / RESX * 100 + camAng:Up() * (1 - y / RESY) * 100
	local camDir = camAng:Forward()
	~~~ lua highlight
	local camX = (1 - 2 * (x + 0.5) / RESX) * camScaleHorizontal
	local camY = (1 - 2 * (y + 0.5) / RESY) * camScaleVertical

	local camDir = Vector(1, camX, camY)
	camDir:Rotate(camAng)
	camDir:Normalize()
	~~~ lua


	~~~ lua delete
	local result = accel:Traverse(origin, camDir)
	~~~ lua highlight
	local result = accel:Traverse(camPos, camDir)
	~~~ lua
	if result then
		return Vector(1, 1, 1)
	else
		return Vector(0, 0, 0)
	end
end
~~~

This should work just fine but lets replace our boring black and white view with one of the best features in VisTrace, automatic texturing
~~~ lua
	if result then
		~~~ lua delete
		return Vector(1, 1, 1)
		~~~ lua highlight
		return result:Albedo()
		~~~ lua
	else
		return Vector(0, 0, 0)
	end
~~~

![The fruits of our labour](./images/vistracing-in-one-weekend/pinhole-camera.png)

The Rendering Equation
----------------------

\begin{equation}
L_o(\mathbf{x}, \omega_o, \lambda, t) = L_e(\mathbf{x}, \omega_o, \lambda, t) + \int_{\Omega} f_s(\mathbf{x}, \omega_i, \omega_o, \lambda, t)L_i(\mathbf{x}, \omega_i, \lambda, t)(\omega_i \cdot \mathbf{n}) \,d\omega_i
\end{equation}

There it is, the rendering equation, the singular equation that underpins all modern renderers. You may be thinking you chose the wrong project to work on this week looking at that, but it's actually far simpler than it looks, and even simpler because most of it we will be ignoring in ray tracing.

To start with lets strip out the symbols we dont need, $\lambda$ is the wavelength of light, which we're abstracting away by using an RGB vector for everything, and $t$ is the current time (not time along the ray, but some time value for animation). This leaves us with the following

\begin{equation}
L_o(\mathbf{x}, \omega_o) = L_e(\mathbf{x}, \omega_o) + \int_{\Omega} f_s(\mathbf{x}, \omega_i, \omega_o)L_i(\mathbf{x}, \omega_i)(\omega_i \cdot \mathbf{n}) \,d\omega_i
\end{equation}

There's still that scary integral in there, but we wont actually be using that until path tracing later on. Instead we're only sampling a single direction at every hit point.

\begin{equation}
L_o(\mathbf{x}, \omega_o) = L_e(\mathbf{x}, \omega_o) + f_s(\mathbf{x}, \omega_i, \omega_o)L_i(\mathbf{x}, \omega_i)(\omega_i \cdot \mathbf{n})
\end{equation}

One more term we can remove is $L_e$, which is the emitted radiance at the hit point. As we wont be sampling emissives directly (well outside the scope of this book) and we wont be bouncing around the scene randomly (path tracing), the only effect we'd get from including this is emissive objects appearing bright when viewed by the camera directly, as well as in reflection and refraction, but they wouldn't light the scene.

\begin{equation}
L_o(\mathbf{x}, \omega_o) = f_s(\mathbf{x}, \omega_i, \omega_o)L_i(\mathbf{x}, \omega_i)(\omega_i \cdot \mathbf{n})
\end{equation}

Now we've cut the equation down to size, lets go over the terms

* $\mathbf{x}$ is the hit point of the ray
* $\omega_o$ is the direction pointing back along the ray ($-\mathbf{d}$ from the ray equation in [1.2](#introduction/tracingaray))
* $\omega_i$ is the direction of scattered light, which in ray tracing is just light samples, reflection, and refraction
* $\mathbf{n}$ is the normal of the surface we hit
* $L_o(\mathbf{x}, \omega_o)$ is the rendering function itself, which calculates the radiance in the outgoing direction $\omega_o$ at the point $\mathbf{x}$
* $f_s(\mathbf{x}, \omega_i, \omega_o)$ is the [bidirectional scattering distribution function](https://en.wikipedia.org/wiki/Bidirectional_scattering_distribution_function), which calculates the amount of light reflected/transmitted from the incoming direction $\omega_i$ in the outgoing direction $\omega_o$. You wont need to worry about the more complex forms of this function, as we'll only be simulating very basic light interaction.
* $L_i(\mathbf{x}, \omega_i)$ is the radiance coming from the incoming direction of light, which is actually the exact same function as $L_o$, except from the hit point and in the direction of incoming light instead of from the camera, making the rendering equation recursive
* $(\omega_i \cdot \mathbf{n})$ is the weakening factor of irradiance due to light smearing over a surface. This will be 1 for our perfect specular reflection and transmission (a detailed explanation of why can be found [here](https://stackoverflow.com/questions/22431912/path-tracing-why-is-there-no-cosine-term-when-calculating-perfect-mirror-reflec))

Armed with this simplified rendering equation, we can now move on to implementing lighting for diffuse surfaces.

Lighting
--------

Before we can light our scene, we need a source of light. There are many types of lights available to use and we can implement any that we want, but here's a few of the easiest

* Point lights are defined by a single point in space (hence the name) and are a good option for simple and versatile lights
* Directional/distance lights emulate the sun by being defined by a direction instead of a point. These are the simplest to implement however produce the least realistic looking lighting
* Area lights are a good alternative to emissives (in fact they're basically how emissives are sampled directly). While a rectangle is the most commonly used form of these, they can be any primitive shape or even a collection of primitives

There are also a few more complex light types

* Emissives are ordinary objects that emit light, and are the most realistic light source. Sampling these however is the hardest, especially when using emissive textures.
* Spot lights are effectively an extension of the point light to emit only in a cone extending from the light. These can also be given soft shadows at the edges, however as they're still an infinitely small point objects will cast hard shadows.
* HDRIs can be used to provide image based lighting. IBL provides both the most realistic lighting information for an environment, as well as being relatively easy to sample and produces minimal noise in the final render

We'll be implementing HDRIs as VisTrace provides functions that completely abstract the details of sampling away from us, and we can use a wide variety of HDRIs for different lighting conditions. This will have the downside of adding a small amount of noise to our ray tracer, but the results will be worth it.

First of all we need a HDRI to load, which I recommend getting from [Poly Haven](https://polyhaven.com/hdris). I'll be using [Drackenstein Quarry](https://polyhaven.com/a/drackenstein_quarry) by [Andreas Mischok](https://www.artstation.com/andreasmischok) throughout the book but you can grab whatever takes your fancy (everything is CC0 licensed!).

Note that while you can download both HDR and EXR formats, VisTrace only supports the HDR format.

Once you have some images you like, place them in `garrysmod/data/vistrace_hdris` so VisTrace can find them. All we have to do now is load one. We also need to create a *sampler* which we'll use in a second
~~~ lua
----------------
--    Init    --
----------------
local accel = vistrace.CreateAccel(ents.FindByClass("prop_*"), false)
~~~ lua highlight
local hdri = vistrace.LoadHDRI("drackenstein_quarry_4k")
local sampler = vistrace.CreateSampler()
~~~

Before we start using this for lighting, lets change our black background to the HDRI
~~~ lua
	local result = accel:Traverse(camPos, camDir)
	if result then
		return result:Albedo()
	else
		~~~ lua delete
		return Vector(0, 0, 0)
		~~~ lua highlight
		return hdri:GetPixel(camDir)
		~~~ lua
	end
~~~

If you look towards the sun you'll notice that everything turns into clown vomit. This is because our HDRIs are *hdr* images, meaning high dynamic range. As colours can be greater than 1 we need to convert them before drawing to our render target that stores 0-1.

We'll use something a little more complex to convert our 0-$\infty$ colours to 0-1 later on, however for now we'll just clamp before drawing to the RT
~~~ lua
		...

		for x = 0, RESX - 1 do
			local rgb = TracePixel(x, y)

			render.SetViewPort(x, y, 1, 1)
			~~~ lua delete
			render.Clear(rgb[1] * 255, rgb[2] * 255, rgb[3] * 255, 255, true, true)
			~~~ lua highlight
			render.Clear(
				math.Clamp(rgb[1] * 255, 0, 255),
				math.Clamp(rgb[2] * 255, 0, 255),
				math.Clamp(rgb[3] * 255, 0, 255),
				255, true, true
			)
			~~~ lua
		end

		...
~~~

Running our script now should render the HDRI whenever a camera ray misses the scene.

That was the easy part, we now need to use the HDRI sampling function along with our simplified rendering equation to determine the colour of the rays that hit.

\begin{equation}
L_o(\mathbf{x}, \omega_o) = f_s(\mathbf{x}, \omega_i, \omega_o)L_i(\mathbf{x}, \omega_i)(\omega_i \cdot \mathbf{n})
\end{equation}

Revisiting the equation we can see we need to calculate the value of $f_s$, $L_i$, and $(\omega_i \cdot \mathbf{n})$ in order to determine our outgoing colour. To start with we'll assume that $f_s$ is 1 and focus on the rest of the equation.

To calculate $L_i$ we can use the `HDRI:Sample()` function to pick a random value for $\omega_i$, we then trace a *shadow ray* out from our hit point in the sampled direction and if it misses the scene, we use the sampled colour. If the sample is invalid or the shadow ray hits an object, no lighting is calculated. We're also going to use the sampler we created earlier as the random number generator for the HDRI sampling function
~~~ lua
	local result = accel:Traverse(camPos, camDir)
	if result then
		~~~ lua delete
		return result:Albedo()
		~~~ lua highlight
		local colour = Vector()

		local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
		if envValid then
			local shadowRay = accel:Traverse(result:Pos(), envDir)
			if not shadowRay then
				colour = ...
			end
		end

		return colour
		~~~ lua
	else
		return hdri:GetPixel(camDir)
	end
~~~

We have our sampled $\omega_i$, as well as a shadow ray so we dont light the scene if the light is blocked. Now we need to compute $L_i$ and $(\omega_i \cdot \mathbf{n})$.

While it seems like we could just use `envCol` for $L_i$, this would heavily *bias* our renderer. The reason for this is because we're only sampling a single direction when we actually want to know all possible directions. This means that the amount of light we're saying is reaching the surface is far lower than it would be if we added together every possible sample direction.

To solve this and *unbias* our renderer, we can divide `envCol` by `envPdf` in order to boost the contribution of samples depending on their probability. We'll go into this in detail in the path tracing chapter, but for now just take it as read
~~~ lua
			if not shadowRay then
				~~~ lua highlight
				local Li = envCol / envPdf
				colour = Li
				~~~ lua
			end
~~~

That's $L_i$ done, next is $(\omega_i \cdot \mathbf{n})$ which is just the dot product between our sampled $\omega_i$ and surface normal $\mathbf{n}$
~~~ lua
			if not shadowRay then
				local Li = envCol / envPdf
				~~~ lua highlight
				colour = Li * envDir:Dot(result:Normal())
				~~~ lua
			end
~~~

If we run this now we should get some extremely noisy lighitng which we'll handle in a second, but we still need to add $f_s$.

In our case this will just be a bidirectional reflectance distribution function, specifically the Lambert BRDF which is about as simple as it gets.
\begin{equation}
f_{lambert} = \frac{albedo}{\pi}
\end{equation}

How this equation is derived is available [here](https://sakibsaikia.github.io/graphics/2019/09/10/Deriving-Lambertian-BRDF-From-First-Principles.html) which you can read if you feel like it, however you wont need to know how BRDFs etc are derived for this book.

Turning that into code is extremely simple
~~~ lua
			if not shadowRay then
				local Li = envCol / envPdf
				~~~ lua highlight
				local bsdf = result:Albedo() / math.pi
				colour = bsdf * li * envDir:Dot(result:Normal())
				~~~ lua
			end
~~~

Congratulations, you just implemented the rendering equation!

We still have a few issues to fix though, well one big issue spread all over our renders. *Noise*.

Our noise is actually a product of two things, the inherent randomness of sampling our HDRI, and something called *shadow acne*.

Shadow acne is an issue caused by floating point precision where the shadow ray starts too close to the surface and immediately intersects with it. Thankfully VisTrace provides a helper function to solve this
~~~ lua
	local result = accel:Traverse(camPos, camDir)
	if result then
		local colour = Vector()
		~~~ lua highlight
		local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())
		~~~ lua

		local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
		if envValid then
			~~~ lua delete
			local shadowRay = accel:Traverse(result:Pos(), envDir)
			~~~ lua highlight
			local shadowRay = accel:Traverse(origin, envDir)
			~~~ lua
			if not shadowRay then
				local Li = envCol / envPdf
				local bsdf = result:Albedo() / math.pi
				colour = bsdf * Li * envDir:Dot(result:Normal())
			end
		end

		return colour
	else
		return hdri:GetPixel(camDir)
	end
~~~

![Woah that makes a difference!](./images/vistracing-in-one-weekend/shadow-acne.png)

The second source of noise, the random sampling, can't be solved with a single function call...

The only way we can reduce the noise of our random sampling (there are others but they're already being used by VisTrace in this instance) is by taking more than one sample.

We'll see this used across our entire renderer when path tracing, but for now we just need to take multiple samples from the HDRI. We can do this by averaging together samples.

This is actually a subset of the integral present in the rendering equation, and this method of approximating the integral by averaging random samples is [Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration), which we'll dig our teeth into when path tracing.

Lets start by adding a new parameter called `SAMPLES` which will be the number of samples to take at each hit
~~~ lua
----------------
-- Parameters --
----------------
local RESX, RESY = 512, 512
~~~ lua highlight
local SAMPLES = 8
~~~ lua

local FOCAL_LENGTH = 60
local SENSOR_HEIGHT = 35
~~~

Then wrap our hdri sampling in a for loop and divide the final colour by the number of samples
~~~ lua
	local result = accel:Traverse(camPos, camDir)
	if result then
		local colour = Vector()
		local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())


		~~~ lua highlight
		for i = 1, SAMPLES do
		~~~ lua
			local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
			if envValid then
				local shadowRay = accel:Traverse(origin, envDir)
				if not shadowRay then
					local brdf = result:Albedo() / math.pi
					local Li = envCol / envPdf
					~~~ lua delete
					colour = brdf * Li * envDir:Dot(result:Normal())
					~~~ lua highlight
					colour = colour + brdf * Li * envDir:Dot(result:Normal())
					~~~ lua
				end
			end
		~~~ lua highlight
		end
		colour = colour / SAMPLES
		~~~ lua

		return colour
	else
		return hdri:GetPixel(camDir)
	end
~~~

One last improvement we can make is discarding invalid samples. The HDRI sampler shouldn't produce many but we don't want to let them contribute to the final image when it does
~~~ lua highlight
		local validSamples = SAMPLES
		~~~ lua
		for i = 1, SAMPLES do
			local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
			if envValid then
				local shadowRay = accel:Traverse(origin, envDir)
				if not shadowRay then
					local brdf = result:Albedo() / math.pi
					local Li = envCol / envPdf
					colour = colour + brdf * Li * envDir:Dot(result:Normal())
				end
			~~~ lua highlight
			else
				validSamples = validSamples - 1
			~~~ lua
			end
		end
		~~~ lua delete
		colour = colour / SAMPLES
		~~~ lua highlight
		colour = colour / validSamples
~~~

![With as few as 32 samples the lighting converges almost completely](./images/vistracing-in-one-weekend/hdri-sampling.png)

Reflection
----------

Refraction
----------

Post Processing
---------------

Conclusion
----------

Path Tracing
============

Revisiting the Rendering Equation
----------------------

\begin{equation}
L_o(\mathbf{x}, \omega_o, \lambda, t) = L_e(\mathbf{x}, \omega_o, \lambda, t) + \int_{\Omega} f_s(\mathbf{x}, \omega_i, \omega_o, \lambda, t)L_i(\mathbf{x}, \omega_i, \lambda, t)(\omega_i \cdot \mathbf{n}) \,d\omega_i
\end{equation}

Brute Force Path Tracing
------------------------

Importance Sampling
-------------------

Next Event Estimation
---------------------

Multiple Importance Sampling
----------------------------

Conclusion
----------

Shading
=======

What is a BSDF?
---------------

Using the VisTrace BSDF
-----------------------

Writing Your Own
----------------

The Microfacet Model
--------------------

Choosing F, D, and G
-------------------

Putting it All Together
-----------------------

Further Reading
===============

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><link rel="stylesheet" href="./css/book.css?v=1"><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
