<head>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

	<title>VisTracing in One Weekend</title>
	<meta name="description" content="Tutorial book on using VisTrace in the style of Ray Tracing in One Weekend">
	<meta name="author" content="Derpius">

	<meta property="og:title" content="VisTracing in One Weekend">
	<meta property="og:type" content="website">
	<meta property="og:url" content="https://derpius.github.io/VisTrace/vistracing-in-one-weekend/index.md.html">
	<meta property="og:description" content="Tutorial book on using VisTrace in the style of Ray Tracing in One Weekend">
	<meta property="og:image" content="https://github.com/Derpius/VisTrace/blob/branding/banner.png?raw=true">

	<!-- Generated with https://realfavicongenerator.net/ -->
	<link rel="apple-touch-icon" sizes="180x180" href="/VisTrace/apple-touch-icon.png?v=2">
	<link rel="icon" type="image/png" sizes="32x32" href="/VisTrace/favicon-32x32.png?v=2">
	<link rel="icon" type="image/png" sizes="16x16" href="/VisTrace/favicon-16x16.png?v=2">
	<link rel="manifest" href="/VisTrace/site.webmanifest?v=2">
	<link rel="mask-icon" href="/VisTrace/safari-pinned-tab.svg?v=2" color="#ee7b0f">
	<link rel="shortcut icon" href="/VisTrace/favicon.ico?v=2">
	<meta name="msapplication-TileColor" content="#343638">
	<meta name="theme-color" content="#343638">

	<link rel="stylesheet" href="style.css?v=1">
</head>

									**VisTracing in One Weekend**
					Inspired by [Ray Tracing in One Weekend](https://raytracing.github.io/books/RayTracingInOneWeekend.html) by [Peter Shirley](https://github.com/petershirley)
					Using illustrations from [Scratchapixel](https://www.scratchapixel.com/)

Introduction
============

Many people have written ray tracers in Garry's Mod, but few have explored the more serious side of light transport, reserving themselves to basic lighting models and guesswork. With VisTracing in One Weekend I hope to provide a gateway into the wider field of light transport, while remaining in GMod and assuming only a minimal understanding of ray tracing.  

By the end of each section you should have a solid groundwork to either move into the next section with, or to develop onto yourself. And by the end of the book you'll have a pretty decent tracer. If you're struggling with something, each section has a link to the source code you'll have by the end of it.

While you could technically follow along without these, the book assumes you'll be using them:
* Garry's Mod
* VisTrace (subscribe to the Workshop addon and download the latest release binary from the GitHub)

You should also already be familiar with Lua (specifically GLua) and basic rendering terminology like shading.  

You'll be implementing the tracer as a simple GLua script run with `lua_openscript_cl` from the console, however you could follow along from Starfall or even write a fully fledged addon around the renderer.  

Drawing an Image
----------------

_[Source code for this section](source-code/1.1.lua)_

Before we get into the nitty gritty of light transport, we need to actually be able to draw pixels to our screen. While there are *many* ways to do this ranging from drawing to a physical screen in the world, to a VGUI frame you can use like a mini application, I've opted for the simplest and most efficient for this step (however you can ignore this and use whatever method you prefer if you're already familiar with rendering in GMod).  

To start with we need a script to work in, and an addon to place that script in. Create a new folder in your `garrysmod/addons` folder with any name you like, then create a subfolder called `lua` and a file in that folder called `vistracer.lua` (you can of course use whatever name you want, however the book assumes this is what you called it). You should now have the file `garrysmod/addons/youraddonname/lua/vistracer.lua`.  

Open the file in your text editor of choice (I recommend vscode with sumneko's Lua extension and the GLua annotations) and add the following

~~~ lua
print("VisTracing in One Weekend!")
~~~

![Console Output](images/console-print.png)

Launch GMod and start singleplayer (or join a multiplayer server with clientside Lua enabled), open console with the grave/backtick key, and enter `lua_openscript_cl vistracer.lua`. You should see `VisTracing in One Weekend!` appear in your console! If not then check that `sv_allowcslua` is set to `1`, and that GMod has been restarted since you created `vistracer.lua` (it only hotloads existing files).  

Now to draw something to the screen we need a render context, and in this case I've chosen the `HUDPaint` hook as a simple way to get pixels above everything except the menu. Remove the print we added and replace it with the hook

~~~ lua
hook.Add("HUDPaint", "VisTracer", function()

end)
~~~  

This wont do anything yet, but go ahead and run it just to check everything's working. We have a render context, but we need to actually draw something. For now lets just use a red rectangle

~~~ lua
hook.Add("HUDPaint", "VisTracer", function()
	~~~ lua highlight
	surface.SetDrawColor(255, 0, 0)
	surface.DrawRect(0, 0, 256, 256)
	~~~ lua
end)
~~~

![`surface.DrawRect`](images/surface-draw.png)

You may be thinking that this is how we'll render an entire image, however if you were to add a print to this hook you'd see that it's called every frame. You can imagine how laggy our game would get if we were ray tracing a scene every frame in Lua.

Instead we need a way to render a set of pixels only once, and save them for future frames. To do this we're going to use *render targets*, which are a special kind of texture we're able to draw to, and the pixels we draw will be saved indefinitely (quite literally in fact, GMod doesn't destroy render targets until the game closes).  

First of all we need to create a render target to use, in addition to a material to assign the render target to as a base texture in order to draw it to the screen later

~~~ lua
local rt = GetRenderTargetEx(
	"VisTracer",                     -- Name of the render target
	1, 1, RT_SIZE_FULL_FRAME_BUFFER, -- Resize to screen res automatically
	MATERIAL_RT_DEPTH_SEPARATE,      -- Create a dedicated depth/stencil buffer
	bit.bor(1, 256),                 -- Texture flags for point sampling and no mips
	0,                               -- No RT flags
	IMAGE_FORMAT_RGBA8888            -- RGB image format with 8 bits per channel
)

local rtMat = CreateMaterial("VisTracer", "UnlitGeneric", {
	["$basetexture"] = rt:GetName(),
	["$translucent"] = "1" -- Enables transparency on the material
})

...
~~~

Next we draw the render target to the screen

~~~ lua
...

hook.Add("HUDPaint", "VisTracer", function()
	~~~ lua delete
	surface.SetDrawColor(255, 0, 0)
	surface.DrawRect(0, 0, 256, 256)
	~~~ lua highlight
	render.SetMaterial(rtMat)
	render.DrawScreenQuad() -- Draws a quad to the entire screen
	~~~ lua
end)
~~~

If you run this with `lua_openscript_cl vistracer.lua` you should get a black screen (don't worry, use `lua_run_cl hook.Remove("HUDPaint", "VisTracer")` to get rid of the hook). This isn't exactly what we want, so lets try drawing some pixels.  

While we could draw to the render target immediately when we run our script, we're going to want to split the drawing over multiple frames for later (ray tracing single threaded in Lua is not fast). To do this we can ignore more complicated (and slower) methods of spreading our code over multiple frames, and just render one row per frame. We also need to define an x and y resolution to draw, which we'll place at the top of the file (this is where all of our parameters will go)

~~~ lua highlight
local RESX, RESY = 256, 256
~~~ lua

...


~~~ lua highlight
local y = 0
~~~ lua
hook.Add("HUDPaint", "VisTracer", function()
	~~~ lua highlight
	if y < RESY then
		render.PushRenderTarget(rt)

		for x = 0, RESX - 1 do
			render.SetViewPort(x, y, 1, 1)
			render.Clear(255, 0, 0, 255, true, true)
		end

		render.PopRenderTarget()
		y = y + 1
	end
	~~~ lua

	render.SetMaterial(rtMat)
	render.DrawScreenQuad()
end)
~~~

Running this should produce a 256x256 red rectangle just like the original `surface.DrawRect` method, except we now have control over the colour of individual pixels and we're only drawing each pixel once. You may also notice that we still have the same problem as before where the entire screen is black except for where we drew our rectangle.

To fix that we'll add a `setup` flag and clear the render target with 0 alpha before we start writing

~~~ lua
...

local y = 0

	~~~ lua highlight
local setup = true
	~~~ lua
hook.Add("HUDPaint", "VisTracer", function()
	if y < RESY then
		render.PushRenderTarget(rt)

		~~~ lua highlight
		if setup then
			render.Clear(0, 0, 0, 0, true, true)
			setup = false
		end
		~~~ lua

		for x = 0, RESX - 1 do
			render.SetViewPort(x, y, 1, 1)
			render.Clear(255, 0, 0, 255, true, true)
		end

		render.PopRenderTarget()
		y = y + 1
	end

	render.SetMaterial(rtMat)
	render.DrawScreenQuad()
end)
~~~

Et voila! You should now have a red rectangle that's significantly slower to draw! Not impressed? Well lets draw something slightly more interesting that we couldn't do with a simple `surface.DrawRect` call...

~~~ lua
		...

		for x = 0, RESX - 1
			render.SetViewPort(x, y, 1, 1)

			~~~ lua delete
			render.Clear(255, 0, 0, 255, true, true)
			~~~ lua

			~~~ lua highlight
			render.Clear(x / (RESX - 1) * 255, y / (RESY - 1) * 255, 0, 255, true, true)
			~~~ lua
		end

		...
~~~

Running our script now should produce a nice gradient from black to red on the x axis, and black to green on the y axis. You may also notice that the pixels are black in the *top left* of the rectangle, and not the bottom left as you might expect.

![Red on the X axis and green on the Y axis](images/xy-square.png)

Tracing a Ray
-------------

_[Source code for this section](source-code/1.2.lua)_

Before we dig into the meat of ray tracing, we need to know how to trace a ray, and to know that we need to know what a "ray" even is.

While it makes sense to think of a ray as a single "ray" of light (light is far more complicated than that in reality), it's actually the total of *all* light that follows a specific line defined by our ray.

Additionally we can define a ray as $\mathbf{x} = \mathbf{o} + t\mathbf{d}$, where $\mathbf{x}$ is the hit point of the ray, $\mathbf{o}$ is the origin, $\mathbf{d}$ is the direction, and $t$ is the *time* or distance along the ray.

Tracing a ray in this context is solving the above equation for the lowest positive value of $t$, which we do by testing every part of the scene geometry for intersections with the ray. We wont have to worry about tracing the ray ourselves, as VisTrace handles this for us, but it's good to know roughly what's going on under the hood.

To trace a ray with VisTrace, it needs to know the scene geometry we want to trace before we can actually trace with it, which we can do by building an *acceleration structure*. The specifics of ray tracing acceleration structures are well outside the scope of this book, but if you want to look into them then VisTrace uses a [bounding volume hierarchy](https://en.wikipedia.org/wiki/Bounding_volume_hierarchy).

Thankfully building an acceleration structure with modern versions of VisTrace is easy, we simply call a function passing in an array of entities we want to intersect with, along with a boolean to toggle intersections with the world (by default it's true). For now we'll just trace any entities whose class starts with `prop_` and disable world tracing. I've also added some comments to help keep our code clean

~~~ lua highlight
----------------
-- Parameters --
----------------
~~~ lua
local RESX, RESY = 256, 256


~~~ lua highlight
----------------
--    Init    --
----------------
local accel = vistrace.CreateAccel(ents.FindByClass("prop_*"), false)
~~~ lua

local rt = GetRenderTargetEx(
	"VisTracer",                     -- Name of the render target
	1, 1, RT_SIZE_FULL_FRAME_BUFFER, -- Resize to screen res automatically
	MATERIAL_RT_DEPTH_SEPARATE,      -- Create a dedicated depth/stencil buffer
	bit.bor(1, 256),                 -- Texture flags for point sampling and no mips
	0,                               -- No RT flags
	IMAGE_FORMAT_RGBA8888            -- RGB image format with 8 bits per channel
)

...
~~~

Now we have our scene geometry in an acceleration structure we can start tracing some rays. While we could put all of this in the render hook from the previous section, to help with readability we're going to create a new function above our hook called `TracePixel`, which will be called from the hook with the x and y coordinates of the current pixel and will return an RGB vector

~~~ lua
...


~~~ lua highlight
local function TracePixel(x, y)
	return Vector(x / (RESX - 1), y / (RESY - 1), 0)
end
~~~ lua

local y = 0
local setup = true
hook.Add("HUDPaint", "VisTracer", function()
	if y < RESY then
		render.PushRenderTarget(rt)
		if setup then
			render.Clear(0, 0, 0, 0, true, true)
			setup = false
		end

		for x = 0, RESX - 1 do
			~~~ lua highlight
			local rgb = TracePixel(x, y)
			~~~ lua

			render.SetViewPort(x, y, 1, 1)
			~~~ lua delete
			render.Clear(x / (RESX - 1) * 255, y / (RESY - 1) * 255, 0, 255, true, true)
			~~~ lua highlight
			render.Clear(rgb[1] * 255, rgb[2] * 255, rgb[3] * 255, 255, true, true)
			~~~ lua
		end

		render.PopRenderTarget()
		y = y + 1
	end

	render.SetMaterial(rtMat)
	render.DrawScreenQuad() -- Draws a quad to the entire screen
end)
~~~

This should produce the exact same image as the end of the last section, however now we can add as much code as we want to `TracePixel` without cluttering the render hook. Also if you're wondering why we're doing `* 255` on the final RGB vector, instead of using 0-255 colours in `TracePixel`, it's because when doing any kind of rendering having `grey * grey` produce 255 (after clamping) is really not helpful.

To trace our ray, we need to know the start position and the direction to trace in. We'll get into generating camera rays in a bit but for now we're going to do [orthographic projection](https://en.wikipedia.org/wiki/Orthographic_projection), meaning all of our rays point in the exact same direction and the origin changes. This has the effect of no matter how close or far away something is, it will always look the same in our final image.

We're also going to use the player camera's origin and angles for our rays, although you could use anything you want

~~~ lua highlight
local camPos, camAng = LocalPlayer():EyePos(), LocalPlayer():EyeAngles()
~~~ lua
local function TracePixel(x, y)
	~~~ lua highlight
	local origin = camPos + camAng:Right() * x / RESX * 100 + camAng:Up() * (1 - y / RESY) * 100
	local camDir = camAng:Forward()

	local result = accel:Traverse(origin, camDir)
	if result then
		return Vector(1, 1, 1)
	else
		return Vector(0, 0, 0)
	end
	~~~ lua delete
	return Vector(x / (RESX - 1), y / (RESY - 1), 0)
	~~~ lua
end
~~~

![Kleiner sitting on a crate](images/orthographic.png)

Because we're just adding onto the player camera's position the rays aren't centred, so you'll need to look down and to the left, but we're now tracing a scene!

Ray Tracing
===========

We have pixels being drawn to the screen, and we have rays being traced, now it's time to simulate light with ray tracing. I wont bore you with a bunch of theory right away, but I will give you an overview of how the ray tracing algorithm works.

We start by generating a ray from a pixel given our camera's properties as well as any additional properties that affect ray generation (like jittering for MSAA), we then trace this ray out into the scene to get a hit point. If we missed it's up to us how we want to colour that pixel, however if we hit an object we plug the information about the hit as well as our ray into the *rendering equation*, which will determine the colour of the hit point that we then assign to the pixel.

We'll be simulating three kinds of light interactions in our ray tracer:
* Perfect diffuse reflection - Light is scattered in roughly all directions from the surface, i.e. Lambertian reflection.
* Perfect specular reflection - Light is scattered entirely in the direction obtained by reflecting our view direction about the surface normal
* Perfect specular transmission - Light is scattered entirely in the direction obtained by refracting our view direction about the surface normal

Generating Camera Rays
----------------------

_[Source code for this section](source-code/2.1.lua)_

Before we start lighting our scene, we should replace the orthographic projection we did earlier with a perspective projection, specifically the [pinhole camera model](https://en.wikipedia.org/wiki/Pinhole_camera_model).

There are a number of ways we can parameterise our pinhole camera model, with some examples being:

- Field of view and resolution
  - This parameterisation is common in games as it's easy to understand
- Focal length, sensor width and height, aspect ratio, and megapixels
  - This lines up well with the specs given by camera manufacturers, but is quite hard to work with
- Focal length, sensor width _or_ height, and resolution
  - This is a reasonable balance between real camera properties and ease of use
  - Usually sensor height is used, as changing the aspect ratio via the resolution will show more or less of your scene horizontally which is generally more intuitive

We'll go with focal length, sensor height and resolution for our tracer and add the new parameters to our code. I've chosen a full frame sensor height of 24mm and a focal length of 60mm fairly arbitrarily, so feel free to play around with them.

~~~ lua
----------------
-- Parameters --
----------------
local RESX, RESY = 256, 256
~~~ lua highlight
local FOCAL_LENGTH_MM = 60
local SENSOR_HEIGHT_MM = 24
~~~

Now how do we apply these parameters to a perspective projection to turn our pixel coordinates into directions for our camera rays? If we take a look at an image of the coordinates we're working with in the pinhole camera model, things become slightly clearer.

![Pinhole camera geometry](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Pinhole.svg/600px-Pinhole.svg.png)

We actually don't care about most of this information though, as all we need to do is translate our pixel coordinates onto that $(y_1,y_2)$ image plane.
We can also ignore that the image plane of a pinhole camera sits _behind_ the origin. Instead, we'll simply place the image plane in front of the origin and trace through it.

The easiest coordinate to calculate is the distance from the origin (where our rays will start) to the image plane as it's simply the focal length we defined earlier. Note that in Source, $x$ is forwards, $y$ is left, and $z$ is up.

\begin{equation}
dir = (f, y_1, y_2)
\end{equation}

We need to do a little more work than that to transform our pixel coordinates to positions $y_1$ and $y_2$ on the image plane.

To start with we'll offset our pixel by $0.5$ so that our coordinate represents the centre of the pixel and not the corner, and then normalise it by dividing by the resolution of the image.

\begin{equation}
normalised_{xy} = \frac{pixel_{xy} + 0.5}{res_{xy}}
\end{equation}

Then we'll transform this $[0,1]$ coordinate to the unit square - $[-1,1]$ - so that it's centred on the origin. Note that whether you subtract the normalised pixel from 1 or 1 from the pixel depends on the handedness of your coordinate system (you will likely need to do this separately for the $x$ and $y$ axis) but this will work for Source.

\begin{equation}
unit_{xy} = 1 - 2 * normalised_{xy}
\end{equation}

Now that we have the pixel transformed to a unit square, we can apply the final transformation to the camera's sensor or image plane.
We do this using the sensor height we defined earlier, calculating the sensor width using the aspect ratio of the image's resolution.

\begin{align}
aspect &= \frac{res_x}{res_y} \\
sensor_w &= sensor_h * aspect
\end{align}

And finally, we multiply the unit coordinate by half of the sensor width and height which gives us our final coordinates.

\begin{equation}
(y_1, y_2) = unit_{xy} * \frac{sensor_{wh}}{2}
\end{equation}

If we expand everything back out again, you'll notice we can simplify it a little further

\begin{align}
(y_1, y_2) &= (1 - 2 * \frac{pixel_{xy} + 0.5}{res_{xy}}) * \frac{sensor_{wh}}{2} \\
(y_1, y_2) &= \frac{sensor_{wh}}{2} - \frac{sensor_{wh}}{2} * 2 * \frac{pixel_{xy} + 0.5}{res_{xy}} \\
(y_1, y_2) &= \frac{sensor_{wh}}{2} - sensor_{wh} * \frac{pixel_{xy} + 0.5}{res_{xy}}
\end{align}

We could simplify even further by expanding $sensor_{w}$ and cancelling part of the aspect ratio with the division by $res_x$,
but this wont really have an affect on the code's performance as we'll be precomputing these values.

That was a lot of maths, but it should hopefully help you understand what our code is doing. Speaking of code, lets implement this.

We'll first precompute the values that wont change over the course of a trace

~~~ lua
----------------
--    Init    --
----------------
local accel = vistrace.CreateAccel(ents.FindByClass("prop_*"), false)


~~~ lua highlight
local sensorWidth = SENSOR_HEIGHT_MM * RESX / RESY

local halfSensorWidth = sensorWidth / 2
local halfSensorHeight = SENSOR_HEIGHT_MM / 2

local sensorWidthDivRes = sensorWidth / RESX
local sensorHeightDivRes = SENSOR_HEIGHT_MM / RESY
~~~

Next, we'll implement our equation in the `TracePixel` function, with the conversion to hammer units defined above

~~~ lua
local camPos, camAng = LocalPlayer():EyePos(), LocalPlayer():EyeAngles()
local function TracePixel(x, y)
	~~~ lua delete
	local origin = camPos + camAng:Right() * x / RESX * 100 + camAng:Up() * (1 - y / RESY) * 100
	local camDir = camAng:Forward()
	~~~ lua highlight
	local camX = halfSensorWidth - sensorWidthDivRes * (x + 0.5)
	local camY = halfSensorHeight - sensorHeightDivRes * (y + 0.5)

	local camDir = Vector(FOCAL_LENGTH_MM, camX, camY)
	camDir:Rotate(camAng)
	camDir:Normalize()
	~~~ lua


	~~~ lua delete
	local result = accel:Traverse(origin, camDir)
	~~~ lua highlight
	local result = accel:Traverse(camPos, camDir)
	~~~ lua
	if result then
		return Vector(1, 1, 1)
	else
		return Vector(0, 0, 0)
	end
end
~~~

This should work just fine, but lets replace our boring black and white view with one of the best features in VisTrace, automatic texturing

~~~ lua
	if result then
		~~~ lua delete
		return Vector(1, 1, 1)
		~~~ lua highlight
		return result:Albedo()
		~~~ lua
	else
		return Vector(0, 0, 0)
	end
~~~

![The fruits of our labour](images/pinhole-camera.png)

If you're wondering why we don't convert from millimetres to hammer units above, it's because we'd scale all coordinates equally just to normalise them which removes that scaling.

The Rendering Equation
----------------------

\begin{equation}
L_o(\mathbf{x}, \omega_o, \lambda, t) = L_e(\mathbf{x}, \omega_o, \lambda, t) + \int_{\Omega} f_s(\mathbf{x}, \omega_i, \omega_o, \lambda, t)L_i(\mathbf{x}, \omega_i, \lambda, t)(\omega_i \cdot \mathbf{n}) \,d\omega_i
\end{equation}

There it is, the rendering equation, the singular equation that underpins all modern renderers. You may be thinking you chose the wrong project to work on this week looking at that, but it's actually far simpler than it looks, and even simpler because most of it we will be ignoring in ray tracing.

To start with lets strip out the symbols we dont need, $\lambda$ is the wavelength of light, which we're abstracting away by using an RGB vector for everything, and $t$ is the current time (not time along the ray, but some time value for animation). This leaves us with the following

\begin{equation}
L_o(\mathbf{x}, \omega_o) = L_e(\mathbf{x}, \omega_o) + \int_{\Omega} f_s(\mathbf{x}, \omega_i, \omega_o)L_i(\mathbf{x}, \omega_i)(\omega_i \cdot \mathbf{n}) \,d\omega_i
\end{equation}

There's still that scary integral in there, but we wont actually be using that until path tracing later on. Instead we're only sampling a single direction at every hit point.

\begin{equation}
L_o(\mathbf{x}, \omega_o) = L_e(\mathbf{x}, \omega_o) + f_s(\mathbf{x}, \omega_i, \omega_o)L_i(\mathbf{x}, \omega_i)(\omega_i \cdot \mathbf{n})
\end{equation}

One more term we can remove is $L_e$, which is the emitted radiance at the hit point. As we wont be sampling emissives directly (well outside the scope of this book) and we wont be bouncing around the scene randomly (path tracing), the only effect we'd get from including this is emissive objects appearing bright when viewed by the camera directly, as well as in reflection and refraction, but they wouldn't light the scene.

\begin{equation}
L_o(\mathbf{x}, \omega_o) = f_s(\mathbf{x}, \omega_i, \omega_o)L_i(\mathbf{x}, \omega_i)(\omega_i \cdot \mathbf{n})
\end{equation}

Now we've cut the equation down to size, lets go over the terms

* $\mathbf{x}$ is the hit point of the ray
* $\omega_o$ is the direction pointing back along the ray ($-\mathbf{d}$ from the ray equation in [1.2](#introduction/tracingaray))
* $\omega_i$ is the direction of scattered light, which in ray tracing is just light samples, reflection, and refraction
* $\mathbf{n}$ is the normal of the surface we hit
* $L_o(\mathbf{x}, \omega_o)$ is the rendering function itself, which calculates the radiance in the outgoing direction $\omega_o$ at the point $\mathbf{x}$
* $f_s(\mathbf{x}, \omega_i, \omega_o)$ is the [bidirectional scattering distribution function](https://en.wikipedia.org/wiki/Bidirectional_scattering_distribution_function), which calculates the amount of light reflected/transmitted from the incoming direction $\omega_i$ in the outgoing direction $\omega_o$. You wont need to worry about the more complex forms of this function, as we'll only be simulating very basic light interaction.
* $L_i(\mathbf{x}, \omega_i)$ is the radiance coming from the incoming direction of light, which is actually the exact same function as $L_o$, except from the hit point and in the direction of incoming light instead of from the camera, making the rendering equation recursive
* $(\omega_i \cdot \mathbf{n})$ is the weakening factor of irradiance due to light smearing over a surface. This will be 1 for our perfect specular reflection and transmission (a detailed explanation of why can be found [here](https://stackoverflow.com/questions/22431912/path-tracing-why-is-there-no-cosine-term-when-calculating-perfect-mirror-reflec))

Armed with this simplified rendering equation, we can now move on to implementing lighting for diffuse surfaces.

Lighting
--------

_[Source code for this section](source-code/2.3.lua)_

Before we can light our scene, we need a source of light. There are many types of lights available to use and we can implement any that we want, but here's a few of the easiest

* Point lights are defined by a single point in space (hence the name) and are a good option for simple and versatile lights
* Directional/distance lights emulate the sun by being defined by a direction instead of a point. These are the simplest to implement however produce the least realistic looking lighting
* Area lights are a good alternative to emissives (in fact they're basically how emissives are sampled directly). While a rectangle is the most commonly used form of these, they can be any primitive shape or even a collection of primitives

There are also a few more complex light types

* Emissives are ordinary objects that emit light, and are the most realistic light source. Sampling these however is the hardest, especially when using emissive textures.
* Spot lights are effectively an extension of the point light to emit only in a cone extending from the light. These can also be given soft shadows at the edges, however as they're still an infinitely small point objects will cast hard shadows.
* HDRIs can be used to provide image based lighting. IBL provides both the most realistic lighting information for an environment, as well as being relatively easy to sample and produces minimal noise in the final render

We'll be implementing HDRIs as VisTrace provides functions that completely abstract the details of sampling away from us, and we can use a wide variety of HDRIs for different lighting conditions. This will have the downside of adding a small amount of noise to our ray tracer, but the results will be worth it.

First of all we need a HDRI to load, which I recommend getting from [Poly Haven](https://polyhaven.com/hdris). I'll be using [Drackenstein Quarry](https://polyhaven.com/a/drackenstein_quarry) by [Andreas Mischok](https://www.artstation.com/andreasmischok) throughout the book but you can grab whatever takes your fancy (everything is CC0 licensed!).

Note that while you can download both HDR and EXR formats, VisTrace only supports the HDR format.

Once you have some images you like, place them in `garrysmod/data/vistrace_hdris` so VisTrace can find them. All we have to do now is load one. We also need to create a *sampler* which we'll use in a second

~~~ lua
----------------
--    Init    --
----------------
local accel = vistrace.CreateAccel(ents.FindByClass("prop_*"), false)
~~~ lua highlight
local hdri = vistrace.LoadHDRI("drackenstein_quarry_4k")
local sampler = vistrace.CreateSampler()
~~~

Before we start using this for lighting, lets change our black background to the HDRI

~~~ lua
	local result = accel:Traverse(camPos, camDir)
	if result then
		return result:Albedo()
	else
		~~~ lua delete
		return Vector(0, 0, 0)
		~~~ lua highlight
		return hdri:GetPixel(camDir)
		~~~ lua
	end
~~~

If you look towards the sun you'll notice that everything turns into clown vomit. This is because our HDRIs are *hdr* images, meaning high dynamic range. As colours can be greater than 1 we need to convert them before drawing to our render target that stores 0-1.

We'll use something a little more complex to convert our 0-$\infty$ colours to 0-1 later on, however for now we'll just clamp before drawing to the RT

~~~ lua
		...

		for x = 0, RESX - 1 do
			local rgb = TracePixel(x, y)

			render.SetViewPort(x, y, 1, 1)
			~~~ lua delete
			render.Clear(rgb[1] * 255, rgb[2] * 255, rgb[3] * 255, 255, true, true)
			~~~ lua highlight
			render.Clear(
				math.Clamp(rgb[1] * 255, 0, 255),
				math.Clamp(rgb[2] * 255, 0, 255),
				math.Clamp(rgb[3] * 255, 0, 255),
				255, true, true
			)
			~~~ lua
		end

		...
~~~

Running our script now should render the HDRI whenever a camera ray misses the scene.

That was the easy part, we now need to use the HDRI sampling function along with our simplified rendering equation to determine the colour of the rays that hit.

\begin{equation}
L_o(\mathbf{x}, \omega_o) = f_s(\mathbf{x}, \omega_i, \omega_o)L_i(\mathbf{x}, \omega_i)(\omega_i \cdot \mathbf{n})
\end{equation}

Revisiting the equation we can see we need to calculate the value of $f_s$, $L_i$, and $(\omega_i \cdot \mathbf{n})$ in order to determine our outgoing colour. To start with we'll assume that $f_s$ is 1 and focus on the rest of the equation.

To calculate $L_i$ we can use the `HDRI:Sample()` function to pick a random value for $\omega_i$, we then trace a *shadow ray* out from our hit point in the sampled direction and if it misses the scene, we use the sampled colour. If the sample is invalid or the shadow ray hits an object, no lighting is calculated. We're also going to use the sampler we created earlier as the random number generator for the HDRI sampling function

~~~ lua
	local result = accel:Traverse(camPos, camDir)
	if result then
		~~~ lua delete
		return result:Albedo()
		~~~ lua highlight
		local colour = Vector()

		local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
		if envValid then
			local shadowRay = accel:Traverse(result:Pos(), envDir)
			if not shadowRay then
				colour = ...
			end
		end

		return colour
		~~~ lua
	else
		return hdri:GetPixel(camDir)
	end
~~~

We have our sampled $\omega_i$, as well as a shadow ray so we dont light the scene if the light is blocked. Now we need to compute $L_i$ and $(\omega_i \cdot \mathbf{n})$.

While it seems like we could just use `envCol` for $L_i$, this would heavily *bias* our renderer. The reason for this is because we're only sampling a single direction when we actually want to know all possible directions. This means that the amount of light we're saying is reaching the surface is far lower than it would be if we added together every possible sample direction.

To solve this and *unbias* our renderer, we can divide `envCol` by `envPdf` in order to boost the contribution of samples depending on their probability. We'll go into this in detail in the path tracing chapter, but for now just take it as read

~~~ lua
			if not shadowRay then
				~~~ lua highlight
				local Li = envCol / envPdf
				colour = Li
				~~~ lua
			end
~~~

That's $L_i$ done, next is $(\omega_i \cdot \mathbf{n})$ which is just the dot product between our sampled $\omega_i$ and surface normal $\mathbf{n}$. We need to use max here as the integral is over the positive hemisphere, but the light direction may be in the negative one

~~~ lua
			if not shadowRay then
				local Li = envCol / envPdf
				~~~ lua delete
				colour = Li
				~~~ lua highlight
				colour = Li * math.max(envDir:Dot(result:Normal()), 0)
				~~~ lua
			end
~~~

If we run this now we should get some extremely noisy lighting which we'll handle in a second, but we still need to add $f_s$.

In our case this will just be a bidirectional reflectance distribution function, specifically the Lambert BRDF which is about as simple as it gets.
\begin{equation}
f_{lambert} = \frac{albedo}{\pi}
\end{equation}

How this equation is derived is available [here](https://sakibsaikia.github.io/graphics/2019/09/10/Deriving-Lambertian-BRDF-From-First-Principles.html) which you can read if you feel like it, however you wont need to know how BRDFs etc are derived for this book.

Turning that into code is extremely simple

~~~ lua
			if not shadowRay then
				~~~ lua highlight
				local brdf = result:Albedo() / math.pi
				~~~ lua
				local Li = envCol / envPdf
				~~~ lua delete
				colour = Li * math.max(envDir:Dot(result:Normal()), 0)
				~~~ lua highlight
				colour = brdf * Li * math.max(envDir:Dot(result:Normal()), 0)
				~~~ lua
			end
~~~

Congratulations, you just implemented the rendering equation!

We still have a few issues to fix though, well one big issue spread all over our renders. *Noise*.

Our noise is actually a product of two things, the inherent randomness of sampling our HDRI, and something called *shadow acne*.

Shadow acne is an issue caused by floating point precision where the shadow ray starts too close to the surface and immediately intersects with it. Thankfully VisTrace provides a helper function to solve this

~~~ lua
	local result = accel:Traverse(camPos, camDir)
	if result then
		local colour = Vector()
		~~~ lua highlight
		local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())
		~~~ lua

		local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
		if envValid then
			~~~ lua delete
			local shadowRay = accel:Traverse(result:Pos(), envDir)
			~~~ lua highlight
			local shadowRay = accel:Traverse(origin, envDir)
			~~~ lua
			if not shadowRay then
				local brdf = result:Albedo() / math.pi
				local Li = envCol / envPdf
				colour = brdf * Li * math.max(envDir:Dot(result:Normal()), 0)
			end
		end

		return colour
	else
		return hdri:GetPixel(camDir)
	end
~~~

![Woah that makes a difference!](images/shadow-acne.png)

The second source of noise, the random sampling, can't be solved with a single function call...

The only way we can reduce the noise of our random sampling (there are others but they're already being used by VisTrace in this instance) is by taking more than one sample.

We'll see this used across our entire renderer when path tracing, but for now we just need to take multiple samples from the HDRI. We can do this by averaging together samples.

This is actually a subset of the integral present in the rendering equation, and this method of approximating the integral by averaging random samples is [Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration), which we'll dig our teeth into when path tracing.

Lets start by adding a new parameter called `SAMPLES` which will be the number of samples to take at each hit

~~~ lua
----------------
-- Parameters --
----------------
local RESX, RESY = 512, 512
~~~ lua highlight
local SAMPLES = 8
~~~ lua

local FOCAL_LENGTH_MM = 60
local SENSOR_HEIGHT_MM = 35
~~~

Then wrap our hdri sampling in a for loop and divide the final colour by the number of samples

~~~ lua
	local result = accel:Traverse(camPos, camDir)
	if result then
		local colour = Vector()
		local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())


		~~~ lua highlight
		for i = 1, SAMPLES do
		~~~ lua
			local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
			if envValid then
				local shadowRay = accel:Traverse(origin, envDir)
				if not shadowRay then
					local brdf = result:Albedo() / math.pi
					local Li = envCol / envPdf
					~~~ lua delete
					colour = brdf * Li * math.max(envDir:Dot(result:Normal()), 0)
					~~~ lua highlight
					colour = colour + brdf * Li * math.max(envDir:Dot(result:Normal()), 0)
					~~~ lua
				end
			end
		~~~ lua highlight
		end

		colour = colour / SAMPLES
		~~~ lua

		return colour
	else
		return hdri:GetPixel(camDir)
	end
~~~

One last improvement we can make is discarding invalid samples. The HDRI sampler shouldn't produce many but we don't want to let them contribute to the final image when it does

~~~ lua highlight
		local validSamples = SAMPLES
		~~~ lua
		for i = 1, SAMPLES do
			local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
			if envValid then
				local shadowRay = accel:Traverse(origin, envDir)
				if not shadowRay then
					local brdf = result:Albedo() / math.pi
					local Li = envCol / envPdf
					colour = colour + brdf * Li * math.max(envDir:Dot(result:Normal()), 0)
				end
			~~~ lua highlight
			else
				validSamples = validSamples - 1
			~~~ lua
			end
		end


		~~~ lua delete
		colour = colour / SAMPLES
		~~~ lua highlight
		colour = colour / validSamples
~~~

![With as few as 32 samples the lighting converges almost completely](images/hdri-sampling.png)

Reflection
----------

_[Source code for this section](source-code/2.4.lua)_

Our diffuse shading looks nice, but could be replicated by a decent rasteriser. To truly showcase the power of ray tracing lets do something that can't be done accurately with rasterising, *specular reflection*.

Specular reflection differs from diffuse reflection in that it's view dependent, it's also one of the two types of light interactions we'll fully explore in our ray tracer (diffuse interreflection is ignored as it's part of path tracing).

While specular reflection comes in many different forms, we're only going to implement perfect (aka delta) reflection for *conductive* surfaces. While we could implement a much wider variety of materials at this stage, this chapter is to prime you for path tracing and not to develop a complex ray tracer.

The details for why conductive and dielectric (non-conductive) materials interact with light differently is complex and not at all important for rendering. The only distinction it will make in our renderer is the type of Fresnel used (more on this later) and whether light not reflected specularly is transmitted or absorbed.

Before we start reflecting any rays we need to make some major modifications to our `TracePixel` function in order to handle multiple bounces. We could handle multiple bounces via recursion, however this is slow and can cause issues later on when you want access to information from the previous bounce.

To start we'll add a new parameter called `MAX_DEPTH` to prevent bouncing too many times

~~~ lua
----------------
-- Parameters --
----------------
local RESX, RESY = 512, 512
local SAMPLES = 32
~~~ lua highlight
local MAX_DEPTH = 8
~~~ lua

local FOCAL_LENGTH_MM = 60
local SENSOR_HEIGHT_MM = 35
~~~

Then we'll wrap our shading code in a loop and our diffuse shading code in an if statement, which for now will always be true as every surface is diffuse

~~~ lua
	local result = accel:Traverse(camPos, camDir)
	if result then
		local colour = Vector()


		~~~ lua highlight
		for depth = 1, MAX_DEPTH do
		~~~ lua
			local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())


			~~~ lua highlight
			if true then
			~~~ lua
				local validSamples = SAMPLES
				for i = 1, SAMPLES do
					local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
					if envValid then
						local shadowRay = accel:Traverse(origin, envDir)
						if not shadowRay then
							local brdf = result:Albedo() / math.pi
							local Li = envCol / envPdf
							colour = colour + brdf * Li * math.max(envDir:Dot(result:Normal()), 0)
						end
					else
						validSamples = validSamples - 1
					end
				end

				colour = colour / validSamples
				~~~ lua highlight
				break
			end
		end
		~~~ lua

		return colour
	else
		return hdri:GetPixel(camDir)
	end
~~~

Now we need a way to determine when a surface is diffuse, and when it's specular.

We'll do this using a very rudimentary method of checking if the material on an entity is `debug/env_cubemap_model`, which is the equivalent of a perfectly reflective surface in Source. This will be replaced with a more advanced system later on

~~~ lua
		for depth = 1, MAX_DEPTH do
			local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())
			~~~ lua highlight
			local diffuse = true
			if result:Entity():IsValid() then
				diffuse = result:Entity():GetMaterial() ~= "debug/env_cubemap_model"
			end
			~~~ lua


			~~~ lua delete
			if true then
			~~~ lua highlight
			if diffuse then
			~~~ lua
				local validSamples = SAMPLES
				for i = 1, SAMPLES do
					local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
					if envValid then
						local shadowRay = accel:Traverse(origin, envDir)
						if not shadowRay then
							local brdf = result:Albedo() / math.pi
							local Li = envCol / envPdf
							colour = colour + brdf * Li * math.max(envDir:Dot(result:Normal()), 0)
						end
					else
						validSamples = validSamples - 1
					end
				end

				colour = colour / validSamples
				break
			~~~ lua highlight
			else
				colour = Vector(1, 0, 0)
				break
			~~~ lua
			end
		end
~~~

This should make all entities with the debug cubemap material appear red.  

The next step is to actually bounce off the surface to achieve reflection. Before adding a vector reflection function though, we'll bounce in the direction of the surface normal to make sure our loop works as we'd expect

~~~ lua
			else
				~~~ lua delete
				colour = Vector(1, 0, 0)
				break
				~~~ lua highlight
				local direction = result:Normal()
				result = accel:Traverse(origin, direction)

				if not result then
					colour = hdri:GetPixel(direction)
					break
				end
				~~~ lua
			end
~~~

![This generates a strange effect that almost looks like specular reflection, but is entirely view independent](images/proto-reflection.png)

We only have a couple things to add now to have conductive reflection, the first of which is an actual vector reflection function.  

As the following images from Scratchapixel show, the angle between the incident vector and the surface normal is equal to the angle between the reflected vector and the surface normal

![The angle of incidence and the angle of reflection are equal](https://www.scratchapixel.com/images/shading-intro/shad-reflection.png)

Additionally, we can express the incident and reflected vectors using $A$ and $B$ like so

![The component vectors A and B of the reflection](https://www.scratchapixel.com/images/shading-intro/shad-reflection2.png)

The equations for the incident vector $I$ and reflected vector $R$ are therefore

\begin{align}
I &= A + B \\
R &= A - B
\end{align}

Where $B$ is equal to

\begin{equation}
B = \cos\theta * N
\end{equation}

Substituting $B$ in the equations for $I$ and $R$ then gives us

\begin{align}
I &= A + \cos\theta * N \\
R &= A - \cos\theta * N
\end{align}

We then rearrange the equation for $I$ to solve for $A$

\begin{equation}
A = I - \cos\theta * N
\end{equation}

And finally we can substitute $A$ in the equation for $R$ and simplify, giving us the vector reflection equation

\begin{align}
R &= I - \cos\theta * N - \cos\theta * N \\
R &= I - 2\cos\theta * N
\end{align}

Now lets implement the above as a function in our tracer. Add a new function above `TracePixel` called `Reflect` which will take the surface normal and our incident vector

~~~ lua
...


~~~ lua highlight
local function Reflect(i, n)
	return i - 2 * i:Dot(n) * n
end
~~~ lua

local camPos, camAng = LocalPlayer():EyePos(), LocalPlayer():EyeAngles()
local function TracePixel(x, y)
	...
end
~~~

Then call this function to calculate a new direction to bounce in `TracePixel`. We'll also add a variable to store the direction that was traced to reach this point

~~~ lua
	...

	local result = accel:Traverse(camPos, camDir)
	if result then
		~~~ lua highlight
		local direction = camDir
		~~~ lua
		local colour = Vector()

		for depth = 1, MAX_DEPTH do
			local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())
			local diffuse = true
			if result:Entity():IsValid() then
				diffuse = result:Entity():GetMaterial() ~= "debug/env_cubemap_model"
			end

			if diffuse then
				local validSamples = SAMPLES
				for i = 1, SAMPLES do
					local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
					if envValid then
						local shadowRay = accel:Traverse(origin, envDir)
						if not shadowRay then
							local brdf = result:Albedo() / math.pi
							local Li = envCol / envPdf
							colour = colour + brdf * Li * math.max(envDir:Dot(result:Normal()), 0)
						end
					else
						validSamples = validSamples - 1
					end
				end

				colour = colour / validSamples
				break
			else
				~~~ lua delete
				local direction = result:Normal()
				~~~ lua highlight
				direction = Reflect(direction, result:Normal())
				~~~ lua
				result = accel:Traverse(origin, direction)

				if not result then
					colour = hdri:GetPixel(direction)
					break
				end
			end
		end

		return colour
	else
		return hdri:GetPixel(camDir)
	end
~~~

Note that we've left `camDir` as a separate variable, this is redundant at the moment but will be useful when we start path tracing.

![Real specular reflection](images/delta-reflection.png)

We have one last step to complete in order to simulate conductors fully though, Fresnel.

The Fresnel equations describe the ratio between reflected and transmitted light, and are completely different for conductive and dielectric surfaces.

However the effect of Fresnel on conductors is extremely subtle, so instead we'll use an approximation common in both real time and offline renderers called [Schlick's approximation](https://en.wikipedia.org/wiki/Schlick%27s_approximation).

\begin{equation}
F(\theta) = F_0 + (1 - F_0)(1 - |\cos\theta|)^5
\end{equation}

Then we can simply use the colour of the surface hit as $F_0$ to achieve an approximate conductive material.

Add a new function called `FresnelSchlicks` after `Reflect`

~~~ lua
local function Reflect(i, n)
	return i - 2 * i:Dot(n) * n
end


~~~ lua highlight
local function FresnelSchlicks(i, n, f0)
	return f0 + (Vector(1, 1, 1) - f0) * math.pow(1 - math.abs(i:Dot(n)), 5)
end
~~~ lua

...
~~~

We now need to make another small change to our rendering code. At the moment we bounce around then set the colour to the first diffuse surface hit, however we want to colour our reflections.

To do this we use a throughput variable that's persistent throughout the ray's journey. This throughput holds the weight of the entire path a ray takes, including all interactions with surfaces along the way

~~~ lua
	...

	local result = accel:Traverse(camPos, camDir)
	if result then
		local direction = camDir
		local colour = Vector()
		~~~ lua highlight
		local throughput = Vector(1, 1, 1)
		~~~ lua

		...
~~~

Then simply multiply throughput by the Fresnel coefficient when we reflect, and multiply colour by the throughput when we terminate the path (diffuse surfaces and missed reflection rays)

~~~ lua
			...

			if diffuse then
				local validSamples = SAMPLES
				for i = 1, SAMPLES do
					local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
					if envValid then
						local shadowRay = accel:Traverse(origin, envDir)
						if not shadowRay then
							local brdf = result:Albedo() / math.pi
							local Li = envCol / envPdf
							colour = colour + brdf * Li * math.max(envDir:Dot(result:Normal()), 0)
						end
					else
						validSamples = validSamples - 1
					end
				end


				~~~ lua delete
				colour = colour / validSamples
				~~~ lua highlight
				colour = colour / validSamples * throughput
				~~~ lua
				break
			else
				direction = Reflect(direction, result:Normal())
				~~~ lua highlight
				local f0 = result:Entity():GetColor()
				throughput = throughput * FresnelSchlicks(
					direction, result:Normal(),
					Vector(f0.r / 255, f0.g / 255, f0.b / 255)
				)
				~~~ lua
				result = accel:Traverse(origin, direction)

				if not result then
					~~~ lua delete
					colour = hdri:GetPixel(direction)
					~~~ lua highlight
					colour = hdri:GetPixel(direction) * throughput
					~~~ lua
					break
				end
			end
~~~

We can't use `result:Albedo()` to get the entity's colour as the debug cubemap material has no base texture, meaning VisTrace would fall back to the missing texture.

![I love gold!](images/conductor.png)

Refraction
----------

_[Source code for this section](source-code/2.5.lua)_

Refraction, or specular transmission, is quite a different beast than specular reflection. In fact you need to both reflect and refract to produce a realistic refractive material with effects like total internal reflection.

We'll need to handle two separate ray paths, front and back surface interactions, a more complex Fresnel, and indices of refraction.

Lets start out the same way we started with reflection by adding a new clause to `TracePixel` for refractive objects. We'll classify all objects marked as specular with the cubemap that have an alpha value below 255 as refractive

~~~ lua
			else
				~~~ lua highlight
				local entColour = result:Entity():GetColor()

				if entColour.a == 255 then
					~~~ lua
					direction = Reflect(direction, result:Normal())
					~~~ lua delete
					local f0 = result:Entity():GetColor()
					~~~ lua
					throughput = throughput * FresnelSchlicks(
						direction, result:Normal(),
						~~~ lua delete
						Vector(f0.r / 255, f0.g / 255, f0.b / 255)
						~~~ lua highlight
						Vector(entColour.r / 255, entColour.g / 255, entColour.b / 255)
						~~~ lua
					)
					result = accel:Traverse(origin, direction)
				~~~ lua highlight
				else
					colour = Vector(1, 0, 0)
					break
				end
				~~~ lua

				if not result then
					colour = hdri:GetPixel(direction) * throughput
					break
				end
			end
~~~

We then need to compute a new origin based on whether we're entering or exiting the surface, and whether it's a front or back hit. We also need to flip the normal if we hit the back of the surface. Additionally lets "refract" by setting the direction to the negative of that upwards facing normal like we did for reflection

~~~ lua
			else
				local entColour = result:Entity():GetColor()

				if entColour.a == 255 then
					direction = Reflect(direction, result:Normal())
					throughput = throughput * FresnelSchlicks(
						direction, result:Normal(),
						Vector(entColour.r / 255, entColour.g / 255, entColour.b / 255)
					)
					result = accel:Traverse(origin, direction)
				else
					~~~ lua highlight
					local upNormal = result:Normal()

					if not result:FrontFacing() then
						upNormal = -upNormal
					end

					origin = not result:FrontFacing() and
						origin or
						vistrace.CalcRayOrigin(
							result:Pos(),
							-result:GeometricNormal()
						)

					direction = -upNormal
					result = accel:Traverse(origin, direction)
					~~~ lua delete
					colour = Vector(1, 0, 0)
					break
					~~~ lua
				end

				if not result then
					colour = hdri:GetPixel(direction) * throughput
					break
				end
			end
~~~

![Equally strange looking transmission to the prototype reflection earlier](images/proto-refraction.png)

Unfortunately the vector refraction function isn't quite as simple as reflection, however the method is the same. We'll construct our transmitted vector using $A$ and $B$ vectors in the plane of incidence as with reflection, except this time the angle between the surface normal and transmitted vector is defined by Snell's law

\begin{equation}
\frac{\sin\theta_I}{\sin{\theta_T}} = \frac{\eta_T}{\eta_I}
\end{equation}

Where $\theta_I$ and $\theta_T$ are the angles between each vector and the normal (inverse of the normal in the case of the transmitted vector), and $\eta_I$ and $\eta_T$ are the indices of refraction for the mediums that the incident and transmitted vectors reside in.

![Illustration of the plane of incidence with reflection and transmission](https://www.scratchapixel.com/images/shading-intro/shad-refraction6.png)

We can then express the transmitted vector $T$ as the sum of vectors $A$ and $B$

\begin{align}
T &= A + B \\
A &= M\sin\theta_T \\
B &= -N\cos\theta_T
\end{align}

![The basis vectors illustrated on a unit circle](https://www.scratchapixel.com/images/shading-intro/shad-refraction7.png)

$N$ is simply the surface normal pointing in the incident direction, and $M$ can be calculated by normalising the vector produced by adding the incident vector $I$ and the height of the indicent vector in the normal direction $C$

\begin{align}
M &= \frac{I + C}{\sin\theta_I} \\
C &= N\cos\theta_I
\end{align}

Then substituting everything in the final equation gives us

\begin{align}
T &= A + B \\
T &= M\sin\theta_T - N\cos\theta_T \\
T &= \frac{(I + C) \sin\theta_T}{\sin\theta_I} - N\cos\theta_T \\
T &= \frac{(I + N\cos\theta_I) \sin\theta_T}{\sin\theta_I} - N\cos\theta_T
\end{align}

Using Snell's law we can remove the unkown $\theta_T$ in the first half of the equation
\begin{align}
\frac{\sin\theta_I}{\sin{\theta_T}} &= \frac{\eta_T}{\eta_I} \\
T &= \frac{\eta_I}{\eta_T}(I + N\cos\theta_I) - N\cos\theta_T
\end{align}

For the second half of the equation, we can use the following trigonometric identity

\begin{align}
\cos^{2}\theta + \sin^{2}\theta &\equiv 1 \\
\cos\theta &\equiv \sqrt{1 - \sin^{2}\theta}
\end{align}

Using that identity we can remove the final unknown $\theta_T$ by substituting $\cos\theta_T$ with the trigonometric identity, replacing $\sin^{2}\theta_T$ with Snell's law. We'll also replace $\sin^{2}\theta_I$ with $(1 - \cos^{2}\theta_I)$ using the same identity in order to not use any slow trignometry functions in code

\begin{align}
\sin{\theta_T} &= \frac{\eta_I}{\eta_T}\sin\theta_I \\
T &= \frac{\eta_I}{\eta_T}(I + N\cos\theta_I) - N\cos\theta_T \\
T &= \frac{\eta_I}{\eta_T}(I + N\cos\theta_I) - N\sqrt{1 - \sin^{2}\theta_T} \\
T &= \frac{\eta_I}{\eta_T}(I + N\cos\theta_I) - N\sqrt{1 - \left( \frac{\eta_I}{\eta_T} \right)^{2}\sin^{2}\theta_I} \\
T &= \frac{\eta_I}{\eta_T}(I + N\cos\theta_I) - N\sqrt{1 - \left( \frac{\eta_I}{\eta_T} \right)^{2}(1 - \cos^{2}\theta_I)}
\end{align}

Finally, we can simplify this equation further, first by defining some coefficients

\begin{align}
\eta^{-1} &= \frac{\eta_I}{\eta_T} \\
c_1 &= \cos\theta_I = -(N \cdot I) \\
c_2 &= \sqrt{1 - \left( \frac{\eta_I}{\eta_T} \right)^{2}(1 - \cos^{2}\theta_I)} = \sqrt{1 - \eta^{-2}(1 - c_1^{2})}
\end{align}

And then substituting in the equation for $T$

\begin{align}
T &= \frac{\eta_I}{\eta_T}(I + N\cos\theta_I) - N\sqrt{1 - \left( \frac{\eta_I}{\eta_T} \right)^{2}(1 - \cos^{2}\theta_I)} \\
T &= \eta^{-1}(I + c_1N) - c_2N \\
T &= \eta^{-1} I + \eta^{-1} c_1N - c_2N \\
T &= \eta^{-1} I + (\eta^{-1} c_1 - c_2)N
\end{align}

Now to implement in code. We'll need to add a special case for when the term in the square root is below 0. This is total internal reflection

~~~ lua
...

local function Reflect(i, n)
	return i - 2 * i:Dot(n) * n
end


~~~ lua highlight
local function Refract(i, n, invEta)
	local c1 = -i:Dot(n)
	local c2sqr = 1 - invEta * invEta * (1 - c1 * c1)

	if c2sqr < 0 then -- Total internal reflection
		return Vector(0, 0, 0)
	end

	return invEta * i + (invEta * c1 - math.sqrt(c2sqr)) * n
end
~~~ lua

local function FresnelSchlicks(i, n, f0)
	return f0 + (Vector(1, 1, 1) - f0) * math.pow(1 - math.abs(i:Dot(n)), 5)
end

...
~~~

Then we can replace our prototype transmission with our new refraction function. I'm using the approximate IoR of air and glass here

~~~ lua
				else
					local upNormal = result:Normal()
					~~~ lua highlight
					local eta = 1.5 / 1
					~~~ lua

					if not result:FrontFacing() then
						upNormal = -upNormal
						~~~ lua highlight
						eta = 1 / eta
						~~~ lua
					end

					origin = not result:FrontFacing() and
						origin or
						vistrace.CalcRayOrigin(
							result:Pos(),
							-result:GeometricNormal()
						)


					~~~ lua delete
					direction = -upNormal
					~~~ lua highlight
					direction = Refract(direction, upNormal, 1 / eta)
					if direction == Vector(0, 0, 0) then
						colour = Vector(1, 0, 0)
						break
					end


					~~~ lua
					result = accel:Traverse(origin, direction)
				end
~~~

![This still looks wrong as it's missing reflection, but the hard part is done](images/delta-refraction.png)

We have a problem though, we need to trace both a reflection and refraction ray but our iterative algorithm only allows exploring a single path
(although it's most likely possible to trace branching paths iteratively, it would be difficult compared to a recursive algorithm).

Tracing both a reflection and refraction path has an additional flaw though, in a scene with many refractive objects close together, each ray will split in half again and again creating an exponentially increasing number of paths.

Additionally, when the Fresnel coefficient is either high or low, we'd be performing the same amount of sampling on diffuse surfaces visible in both reflection and refraction paths despite one having a far lower impact on the final image.

The solution to all of these problems is to stochastically sample either a reflection or refraction path using the Fresnel coefficient. Recall when we implemented conductors that Fresnel describes the amount of light reflected, meaning we can use it as the probability a ray is reflected.

A downside with this method is that to implement this sampling effectively into our rendering algorithm, we'll need to sample the entire path minus the camera ray again and again. This will cause our delta reflection only conductors to be far less efficient, however this method is by far the best for any other surface type including rough reflections and diffuse reflection when we start path tracing.

First of all we need to drastically modify our `TracePixel` function once again to sample the entire path instead of only diffuse direct lighting

~~~ lua
local camPos, camAng = LocalPlayer():EyePos(), LocalPlayer():EyeAngles()
local function TracePixel(x, y)
	local camX = halfSensorWidth - sensorWidthDivRes * (x + 0.5)
	local camY = halfSensorHeight - sensorHeightDivRes * (y + 0.5)

	local camDir = Vector(FOCAL_LENGTH_MM, camX, camY)
	camDir:Rotate(camAng)
	camDir:Normalize()

	~~~ lua delete
	local result = accel:Traverse(camPos, camDir)
	if result then
	~~~ lua highlight
	local camRay = accel:Traverse(camPos, camDir)
	if camRay then
		~~~ lua delete
		local direction = camDir
		~~~ lua
		local colour = Vector()


		~~~ lua highlight
		local validSamples = SAMPLES
		for sample = 1, SAMPLES do
			local result = camRay
			local direction = camDir
			~~~ lua
			local throughput = Vector(1, 1, 1)

			for depth = 1, MAX_DEPTH do
				local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())
				local diffuse = true
				if result:Entity():IsValid() then
					diffuse = result:Entity():GetMaterial() ~= "debug/env_cubemap_model"
				end

				if diffuse then
					~~~ lua delete
					local validSamples = SAMPLES
					for i = 1, SAMPLES do
					~~~ lua
					local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
					if envValid then
						local shadowRay = accel:Traverse(origin, envDir)
						if not shadowRay then
							local brdf = result:Albedo() / math.pi
							local Li = envCol / envPdf
							~~~ lua delete
							colour = colour + brdf * Li * math.max(envDir:Dot(result:Normal()), 0)
							~~~ lua highlight
							local integral = brdf * Li * math.max(envDir:Dot(result:Normal()), 0)
							colour = colour + integral * throughput
							~~~ lua
						end
					else
						validSamples = validSamples - 1
					end
					~~~ lua delete
					end

					colour = colour / validSamples * throughput
					~~~ lua
					break
				else
					local entColour = result:Entity():GetColor()

					if entColour.a == 255 then
						direction = Reflect(direction, result:Normal())
						throughput = throughput * FresnelSchlicks(
							direction, result:Normal(),
							Vector(entColour.r / 255, entColour.g / 255, entColour.b / 255)
						)
						result = accel:Traverse(origin, direction)
					else
						local upNormal = result:Normal()
						local eta = 1.5 / 1

						if not result:FrontFacing() then
							upNormal = -upNormal
							eta = 1 / eta
						end

						origin = not result:FrontFacing() and
							origin or
							vistrace.CalcRayOrigin(
								result:Pos(),
								-result:GeometricNormal()
							)

						direction = Refract(direction, upNormal, 1 / eta)
						if direction == Vector(0, 0, 0) then
							~~~ lua delete
							colour = Vector(1, 0, 0)
							~~~ lua highlight
							colour = colour + Vector(1, 0, 0)
							~~~ lua
							break
						end

						result = accel:Traverse(origin, direction)
					end

					if not result then
						~~~ lua delete
						colour = hdri:GetPixel(direction) * throughput
						~~~ lua highlight
						colour = colour + hdri:GetPixel(direction) * throughput
						~~~ lua
						break
					end
				end
			end
		~~~ lua highlight
		end
		~~~ lua


		~~~ lua delete
		return colour
		~~~ lua highlight
		return colour / validSamples
		~~~ lua
	else
		return hdri:GetPixel(camDir)
	end
end
~~~

That's quite a few changes, so double check each deletion and addition if anything isn't working after this step.

All going well you should have the exact same renders as before, however both conductors and our half implemented specular transmissive materials will render far slower.

Now we can stochastically sample reflection and refraction in our transmissive material. Lets test with a hardcoded 20% chance of reflection

~~~ lua
					else
						local upNormal = result:Normal()
						local eta = 1.5 / 1

						if not result:FrontFacing() then
							upNormal = -upNormal
							eta = 1 / eta
						end


						~~~ lua highlight
						local reflection = sampler:GetFloat() < 0.2
						~~~ lua delete
						origin = not result:FrontFacing() and
						~~~ lua highlight
						origin = reflection == result:FrontFacing() and
						~~~ lua
							origin or
							vistrace.CalcRayOrigin(
								result:Pos(),
								-result:GeometricNormal()
							)


						~~~ lua delete
						direction = Refract(direction, upNormal, 1 / eta)
						~~~ lua highlight
						direction = reflection and
							Reflect(direction, upNormal) or
							Refract(direction, upNormal, 1 / eta)


						~~~ lua
						if direction == Vector(0, 0, 0) then
							colour = colour + Vector(1, 0, 0)
							break
						end

						result = accel:Traverse(origin, direction)
					end
~~~

![That definitely looks a lot more like a real world material than before](images/proto-reflection-transmission.png)

Now while we could use the nice and simple Schlick's approximation here too, the effect of Fresnel on dielectrics is far more apparent, so we'll implement the full dielectric Fresnel.

The equations for each polarity are taken from [this blog post](https://seblagarde.wordpress.com/2013/04/29/memo-on-fresnel-equations/) by Sebastien Lagarde. We're entirely ignoring the polarisation of light so the final coefficient is just the average of the two

\begin{align}
r_\bot &= \frac{\cos\theta - \eta\sqrt{1 - \eta^{-2}(1 - \cos^{2}\theta)}}{\cos\theta + \eta\sqrt{1 - \eta^{-2}(1 - \cos^{2}\theta)}} \\
r_\| &= \frac{\sqrt{1 - \eta^{-2}(1 - \cos^{2}\theta)} - \eta\cos\theta}{\sqrt{1 - \eta^{-2}(1 - \cos^{2}\theta)} + \eta\cos\theta} \\
R &= \frac{r_\bot^{2} + r_\|^{2}}{2}
\end{align}

You may notice that a few of these terms are repeated so we'll save them to some temporary variables. We'll also use the same trigonometric identity trick we used in the refraction function in order to avoid calling `math.sin`

~~~ lua
...

local function FresnelSchlicks(i, n, f0)
	return f0 + (Vector(1, 1, 1) - f0) * math.pow(1 - math.abs(i:Dot(n)), 5)
end


~~~ lua highlight
local function FresnelDielectric(i, n, eta)
	local c1 = -i:Dot(n)
	local c2sqr = 1 - (1 - c1 * c1) / (eta * eta)

	if c2sqr < 0 then -- Total internal reflection
		return 1
	end

	local c2 = math.sqrt(c2sqr)
	local c3 = eta * c2
	local c4 = eta * c1

	local rs = (c1 - c3) / (c1 + c3)
	local rp = (c2 - c4) / (c2 + c4)

	return 0.5 * (rs * rs + rp * rp)
end
~~~ lua

...
~~~

We can then replace our hardcoded reflection probability with this function like so

~~~ lua
						...

						~~~ lua delete
						local reflection = sampler:GetFloat() < 0.2
						~~~ lua highlight
						local reflection = sampler:GetFloat() < FresnelDielectric(direction, upNormal, eta)
						~~~ lua
						origin = reflection == result:FrontFacing() and
							origin or
							vistrace.CalcRayOrigin(
								result:Pos(),
								-result:GeometricNormal()
							)

						...
~~~

You may be wondering why we're not adjusting the throughput here. The contribution of the reflection and refraction paths are $\text{Fresnel}$ and $1 - \text{Fresnel}$ respectively, however those are also the probability that each path was sampled. This is actually a taste of what we'll see a lot more of in path tracing, where we divide the contribution of a path by the probability that it was sampled.

As the probability is the same as the contribution of each path in this instance, dividing them by each other produces $1$, leaving the throughput unchanged.

This may not make intuitive sense if you're not already familiar with statistics, however I will explain it in more detail when we start path tracing. For now, admire your work.

![A display of all facets of a specular transmissive material](images/fresnel.png)

Post Processing
---------------

_[Source code for this section](source-code/2.6.lua)_

The last feature we'll add to our ray tracer is post processing. This is a good cooldown after the fairly maths heavy section on refraction.

At the moment we're rendering our raw HDR render data out to the screen using a clamp, however the proper way to convert HDR to a standard monitor's sRGB colour space is to perform tonemapping and linear to sRGB conversion.

These are both post processing steps as they're applied after we render the scene. A few other examples are FXAA, upscaling, and denoising.

We could implement some of these with relative ease in Lua, however VisTrace provides a custom render target class that has various methods including ACES fitted tonemapping (with simple auto exposure and linear to sRGB conversion) and saving to disk. In addition, VisTrace extensions can add more render target methods to provide extended functionality [like denoising](https://github.com/yogwoggf/gmdenoiser).

To start we'll create a VisTrace RT to write our linear HDR data to

~~~ lua
----------------
--    Init    --
----------------
local accel = vistrace.CreateAccel(ents.FindByClass("prop_*"), false)
local hdri = vistrace.LoadHDRI("drackenstein_quarry_4k")
local sampler = vistrace.CreateSampler()


~~~ lua highlight
local hdr = vistrace.CreateRenderTarget(RESX, RESY, VisTraceRTFormat.RGBFFF)
~~~ lua

...
~~~

Then we'll write the vector returned by `TracePixel` to our new RT, and save the RT to disk when we finish rendering the image

~~~ lua
local y = 0
local setup = true
hook.Add("HUDPaint", "VisTracer", function()
	if y < RESY then
		render.PushRenderTarget(rt)
		if setup then
			render.Clear(0, 0, 0, 0, true, true)
			setup = false
		end

		for x = 0, RESX - 1 do
			local rgb = TracePixel(x, y)
			~~~ lua highlight
			hdr:SetPixel(x, y, rgb)
			~~~ lua

			render.SetViewPort(x, y, 1, 1)
			render.Clear(
				math.Clamp(rgb[1] * 255, 0, 255),
				math.Clamp(rgb[2] * 255, 0, 255),
				math.Clamp(rgb[3] * 255, 0, 255),
				255, true, true
			)
		end

		render.PopRenderTarget()
		y = y + 1


		~~~ lua highlight
		if y >= RESY then
			hdr:Save("render.png")
		end
		~~~ lua
	end

	render.SetMaterial(rtMat)
	render.DrawScreenQuad() -- Draws a quad to the entire screen
end)
~~~

Now we can add a call to `Tonemap` before we save the image. I'm passing `true` to enable auto exposure here, and you can pass a number to the second parameter to set an exposure offset giving you some minimal level of control.

~~~ lua
		if y >= RESY then
			~~~ lua highlight
			hdr:Tonemap(true)
			~~~ lua
			hdr:Save("render.png")
		end
~~~

![From oversaturated and blown out highlights to Blender render in a few lines of code](images/post-processing.png)

Conclusion
----------

200 lines later and we have a fairly complete ray tracer showcasing conductive specular reflection and specular reflection transmission, in addition to sampled diffuse direct lighting that could be expanded to point, distant, and area lights with ease.

At this point you should have a grasp on the core concepts of light transport, as well as being more comfortable with implementing mathematical equations in code. You may also want to re-read everything up to this point to review anything you might not have understood on the first pass.

You could continue to build on this base with various additional features, or you can move on to path tracing, which will explore light transport in far more detail, doing away with many of the abstractions we made when ray tracing.

You can also share any renders you make on the [VisTrace discussions show and tell page](https://github.com/Derpius/VisTrace/discussions/categories/show-and-tell), and if you want any to be featured on the VisTrace workshop or future branding then you can submit them via the [showcase issue template](https://github.com/Derpius/VisTrace/issues/new/choose) or as pull request to the branding branch.

If you submit renders to the showcase then please name them `{author},{description}.png`, this allows propper attribution to be generated automatically.  

You can find the full source code for this chapter [here](source-code/2.6.lua)

![Obligatory Cornell box render](images/ray-tracer.png)

Path Tracing
============

Path tracing is an extension of the ray tracing algorithm that explores all interactions of light with a surface. It does this by using Monte Carlo integration to approximate the rendering equation's integral over the hemisphere.

By exploring all light paths fully (as opposed to only specular reflection and transmission in our ray tracer) path tracing can produce images indistinguishable from reality when given a physically accurate scene, at the cost of significantly higher variance.

While path tracing requires us to correctly implement more complex maths to prevent severe artefacts, the generalisation of light means we no longer need to handle various materials separately as any changes in our BSDF will adjust the light paths accordingly.

At the end of this chapter you should have a much deeper understanding of the rendering equation and Monte Carlo integration, along with an unbiased path tracer using the VisTrace BSDF.

Revisiting the Rendering Equation
----------------------

\begin{equation}
L_o(\mathbf{x}, \omega_o, \lambda, t) = L_e(\mathbf{x}, \omega_o, \lambda, t) + \int_{\Omega} f_s(\mathbf{x}, \omega_i, \omega_o, \lambda, t)L_i(\mathbf{x}, \omega_i, \lambda, t)(\omega_i \cdot \mathbf{n}) \,d\omega_i
\end{equation}

Back to the rendering equation, and this time we can't strip everything out.

\begin{equation}
L_o(\mathbf{x}, \omega_o) = L_e(\mathbf{x}, \omega_o) + \int_{\Omega} f_s(\mathbf{x}, \omega_i, \omega_o)L_i(\mathbf{x}, \omega_i)(\omega_i \cdot \mathbf{n}) \,d\omega_i
\end{equation}

All we can do is remove the unused time value and implicit light wavelength. I've left emission in as although we wont be implementing it in this chapter due to VisTrace not currently having a direct light sampler for emissives, you are able to easily add naive emissive lighting.

The path tracing algorithm works by performing Monte Carlo integration on the integral, which describes the continuous sum of irradiance from all points on the hemisphere at the surface interaction $\mathbf{x}$.

Monte Carlo integration itself works by approximating the integral using the mean of random samples taken from said integral, weighted by their probability

\begin{equation}
\lim_{N \to +\infty} \frac{1}{N} \sum_{i = 1}^{N} \frac{f(\mathbf{x}_i)}{p(\mathbf{x}_i)} = \int_{\Omega} f(\mathbf{x}) \,d\mathbf{x}
\end{equation}

To understand the division by the probability of the sample, imagine we have coin that has a number written on each side, and we want to calculate the total obtained from adding both sides together. We could of course look at both sides of the coin, but for the purpose of this exercise that is impossible to do.

The only way we can determine the sum of both sides of the coin is by flipping it, so we need to approximate it given the random sides we'll see.

Our first thought might be to just take a few samples and average them together

\begin{equation}
\lim_{N \to +\infty} \frac{1}{N} \sum_{i = 1}^{N} \mathbf{x}_i = \mathbf{x}_a + \mathbf{x}_b
\end{equation}

However if we take an example where we draw two samples that happen to be one of each side, you can see that this is incorrect

\begin{equation}
\frac{1}{2} (\mathbf{x}_a + \mathbf{x}_b) = \mathbf{x}_a + \mathbf{x}_b
\end{equation}

The solution is to divide each sample we take by the probability of the sample, so that when we average them together we're adding back in an approximation of the information we're missing by only looking at a single side.

\begin{align}
p(\mathbf{x}) &= 0.5 \\
\lim_{N \to +\infty} \frac{1}{N} \sum_{i = 1}^{N} \frac{\mathbf{x}_i}{p(\mathbf{x}_i)} &= \mathbf{x}_a + \mathbf{x}_b
\end{align}

Again with an example of sampling one of each side, the mean and weights cancel out. Of course these wont cancel perfectly unless you draw an infinite number of samples (in this case we can use a finite number as the sum itself is finite)

\begin{align}
\frac{1}{2} (\frac{\mathbf{x}_a}{0.5} + \frac{\mathbf{x}_b}{0.5}) &= \mathbf{x}_a + \mathbf{x}_b \\
\frac{\mathbf{x}_a}{2 * 0.5} + \frac{\mathbf{x}_b}{2 * 0.5} &= \mathbf{x}_a + \mathbf{x}_b
\end{align}

We actually already used Monte Carlo integration in the ray tracer for both diffuse direct lighting (which was sampling a region of the rendering equation's integral), and for Fresnel where we implemented what is basically the above example except the probability of $\mathbf{x}_a$ and $\mathbf{x}_b$ was determined by the coefficient.

Finally, we can replace the rendering equation's integral with Monte Carlo

\begin{equation}
L(\mathbf{x}_d, \omega_o) = L_e(\mathbf{x}_d, \omega_o) + \lim_{N \to +\infty} \sum_{i = 1}^{N} \frac{f_s(\mathbf{x}_d, \omega_i, \omega_o)L(\mathbf{x}_{d + 1}, \omega_i)(\omega_i \cdot \mathbf{n})}{PDF(\omega_i)}
\end{equation}

I've renamed some terms to make the recursive nature of the rendering equation more obvious, as well as showing incrementing the path vertex $\mathbf{x}$ as we traverse the scene. Also $PDF$ (probability distribution function) is just the technical name for the $p$ function in the Monte Carlo example.

Naive Path Tracing
------------------------

_[Source code for this section](source-code/3.2.lua)_

Now that we've got the theory out of the way it's time to implement.

Before we start writing code we need to remove all of the existing lighting calculations from our `TracePixel` function. You should save a copy of your finished ray tracer in case you want to revisit it later

~~~ lua
...

local camPos, camAng = LocalPlayer():EyePos(), LocalPlayer():EyeAngles()
local function TracePixel(x, y)
	local camX = halfSensorWidth - sensorWidthDivRes * (x + 0.5)
	local camY = halfSensorHeight - sensorHeightDivRes * (y + 0.5)

	local camDir = Vector(FOCAL_LENGTH_MM, camX, camY)
	camDir:Rotate(camAng)
	camDir:Normalize()

	local camRay = accel:Traverse(camPos, camDir)
	if camRay then
		local colour = Vector()

		local validSamples = SAMPLES
		for sample = 1, SAMPLES do
			local result = camRay
			~~~ lua delete
			local direction = camDir
			~~~ lua
			local throughput = Vector(1, 1, 1)

			for depth = 1, MAX_DEPTH do
				local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())
				~~~ lua delete
				local diffuse = true
				if result:Entity():IsValid() then
					diffuse = result:Entity():GetMaterial() ~= "debug/env_cubemap_model"
				end

				if diffuse then
					local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
					if envValid then
						local shadowRay = accel:Traverse(origin, envDir)
						if not shadowRay then
							local brdf = result:Albedo() / math.pi
							local Li = envCol / envPdf
							local integral = brdf * Li * math.max(envDir:Dot(result:Normal()), 0)
							colour = colour + integral * throughput
						end
					else
						validSamples = validSamples - 1
					end

					break
				else
					local entColour = result:Entity():GetColor()

					if entColour.a == 255 then
						direction = Reflect(direction, result:Normal())
						throughput = throughput * FresnelSchlicks(
							direction, result:Normal(),
							Vector(entColour.r / 255, entColour.g / 255, entColour.b / 255)
						)
						result = accel:Traverse(origin, direction)
					else
						local upNormal = result:Normal()
						local eta = 1.5 / 1

						if not result:FrontFacing() then
							upNormal = -upNormal
							eta = 1 / eta
						end

						local reflection = sampler:GetFloat() < FresnelDielectric(direction, upNormal, eta)
						origin = reflection == result:FrontFacing() and
							origin or
							vistrace.CalcRayOrigin(
								result:Pos(),
								-result:GeometricNormal()
							)

						direction = reflection and
							Reflect(direction, upNormal) or
							Refract(direction, upNormal, 1 / eta)

						if direction == Vector(0, 0, 0) then
							colour = colour + Vector(1, 0, 0)
							break
						end

						result = accel:Traverse(origin, direction)
					end

					if not result then
						colour = colour + hdri:GetPixel(direction) * throughput
						break
					end
				end
				~~~ lua
			end
		end

		return colour / validSamples
	else
		return hdri:GetPixel(camDir)
	end
end

...
~~~

As you can see, the way we implemented sampling into the ray tracer allowed us to easily reuse the core logic.

We should also remove our shading functions as this will all be handled by the VisTrace BSDF later on

~~~ lua
...


~~~ lua delete
local function Reflect(i, n)
	return i - 2 * i:Dot(n) * n
end

local function Refract(i, n, invEta)
	local c1 = -i:Dot(n)
	local c2sqr = 1 - invEta * invEta * (1 - c1 * c1)

	if c2sqr < 0 then -- Total internal reflection
		return Vector(0, 0, 0)
	end

	return invEta * i + (invEta * c1 - math.sqrt(c2sqr)) * n
end

local function FresnelSchlicks(i, n, f0)
	return f0 + (Vector(1, 1, 1) - f0) * math.pow(1 - math.abs(i:Dot(n)), 5)
end

local function FresnelDielectric(i, n, eta)
	local c1 = -i:Dot(n)
	local c2sqr = 1 - (1 - c1 * c1) / (eta * eta)

	if c2sqr < 0 then -- Total internal reflection
		return 1
	end

	local c2 = math.sqrt(c2sqr)
	local c3 = eta * c2
	local c4 = eta * c1

	local rs = (c1 - c3) / (c1 + c3)
	local rp = (c2 - c4) / (c2 + c4)

	return 0.5 * (rs * rs + rp * rp)
end
~~~ lua

...
~~~

Now we already know how to compute a reflected colour iteratively using the throughput value, and we know how to weight the throughput as the ray propagates the scene. All we have to do is figure out how to pick a sample from the integral.

The simplest way to do this is to uniformly sample a point on the hemisphere, but how exactly?

I have seen many people attempt to sample the hemisphere by picking a random value for $x$ and $y$ in the range $[-1,1]$ and $z$ in the range $[0,1]$, then normalising. This generates severe biasing towards the corners of the sampled range as can be seen in the image below.

![A heatmap of this sampling strategy's PDF I made with matplotlib](images/randvec-sampling.png)

The correct method is to sample a height $z$ in the range $[0,1]$ and an azimuthal angle $\phi$ in the range $[0, 2\pi)$. We can then calculate a Cartesian coordinate using spherical coordinates, where the azimuth is the value we sampled, and the polar angle $\theta$ is calculated from the height.

A Cartesian coordinate can be calculated from spherical coordinates like so (where $z$ is the polar axis and the radius is $1$)

\begin{align}
x &= \sin\theta \cos\phi \\
y &= \sin\theta \sin\phi \\
z &= \cos\theta
\end{align}

We can use the trigonometric identity from earlier once again to calculate $\sin\theta$

\begin{align}
\sin\theta &\equiv \sqrt{1 - \cos^{2}\theta} \\
\sin\theta &\equiv \sqrt{1 - z^{2}}
\end{align}

This gives us

\begin{align}
z &= \xi_1 \\
\phi &= 2 \pi \xi_2 \\
x &= \sqrt{1 - z^{2}} \cos\phi \\
y &= \sqrt{1 - z^{2}} \sin\phi
\end{align}

Where $\xi_1$ and $\xi_2$ are uniform random variables in the range $[0,1)$. This doesn't match the range we want for $z$, however I have yet to see an implementation handle this subtle difference.

We can implement this in code like so

~~~ lua
...

local rtMat = CreateMaterial("VisTracer", "UnlitGeneric", {
	["$basetexture"] = rt:GetName(),
	["$translucent"] = "1" -- Enables transparency on the material
})


~~~ lua highlight
local function SampleHemisphere(r1, r2)
	local sinTheta = math.sqrt(1 - r1 * r1)
	local phi = 2 * math.pi * r2

	return Vector(
		sinTheta * math.cos(phi),
		sinTheta * math.sin(phi),
		r1
	)
end
~~~ lua

local camPos, camAng = LocalPlayer():EyePos(), LocalPlayer():EyeAngles()
local function TracePixel(x, y)
	...
end
~~~

Before we can trace however, we need to transform the local z-up hemisphere direction to world space, which is easy to do using the normal, tangent, and binormal vectors stored in trace results.

We can use vector construction as we did with reflection and refraction in order to produce a world space value

\begin{align}
x_{\text{world}} &= T x_{\text{local}} \\
y_{\text{world}} &= B y_{\text{local}} \\
z_{\text{world}} &= N z_{\text{local}}
\end{align}

~~~ lua delete
local function SampleHemisphere(r1, r2)
~~~ lua highlight
local function SampleHemisphere(result, r1, r2)
~~~ lua
	local sinTheta = math.sqrt(1 - r1 * r1)
	local phi = 2 * math.pi * r2


	~~~ lua delete
	return Vector(
		sinTheta * math.cos(phi),
		sinTheta * math.sin(phi),
		r1
	~~~ lua highlight
	return (
		sinTheta * math.cos(phi) * result:Tangent() +
		sinTheta * math.sin(phi) * result:Binormal() +
		r1                       * result:Normal()
	~~~ lua
	)
end
~~~

Then we can add a call to this function in `TracePixel`, as well as some demo code to see if it works

~~~ lua
			...

			for depth = 1, MAX_DEPTH do
				~~~ lua highlight
				local sample = SampleHemisphere(result, sampler:GetFloat2D())
				~~~ lua

				local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())


				~~~ lua highlight
				result = accel:Traverse(origin, sample)
				if not result then
					colour = colour + hdri:GetPixel(sample) * throughput
					break
				end
				~~~ lua
			end
~~~

![Notice how the HDRI is lighting the sphere without us explicitly sampling it](images/uniform-sampling.png)

Now we can add in evaluation of the integral. This is the exact same equation that we evaluated when sampling diffuse lighting before, except the $L_i$ term is removed as this is added when we do `colour = colour + throughput * ...`

\begin{equation}
\frac{f_s(\mathbf{x}_d, \omega_i, \omega_o)(\omega_i \cdot \mathbf{n})}{PDF(\omega_i)}
\end{equation}

We will once again use the Lambert BRDF $\frac{\text{albedo}}{\pi}$, and the $PDF$ of our uniform sampling function is simply $1$ divided by the surface area of a unit hemisphere, $2 \pi$

\begin{equation}
\frac{\frac{\text{albedo}}{\pi}(\omega_i \cdot \mathbf{n})}{\frac{1}{2 \pi}} = \frac{\text{albedo}}{\pi}(\omega_i \cdot \mathbf{n}) * 2 \pi = 2 * \text{albedo} * (\omega_i \cdot \mathbf{n})
\end{equation}

~~~ lua
			for depth = 1, MAX_DEPTH do
				local sample = SampleHemisphere(result, sampler:GetFloat2D())
				~~~ lua highlight
				throughput = throughput * 2 * result:Albedo() * sample:Dot(result:Normal())
				~~~ lua

				local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())

				result = accel:Traverse(origin, sample)
				if not result then
					colour = colour + hdri:GetPixel(sample) * throughput
					break
				end
			end
~~~

Note that unlike when directly sampling the HDRI, we don't need to `math.max` the weakening term as the sample is guaranteed to be in the positive hemisphere. Also this cancelling of terms between the BSDF and PDF is something you'll encounter very frequently.

![Naive path tracing](images/uniform-path-tracing.png)

Well that was surprisingly easy, other than the more in depth theory, but it doesn't stop there...

Importance Sampling
-------------------

_[Source code for this section](source-code/3.3.lua)_

At the moment we're uniformly selecting a point on the hemisphere, but if we look at our evaluation the throughput is higher when the dot product between the sampled vector and surface normal is higher.

Importance sampling is choosing samples based on how much they'll contribute to the image, i.e. the relative contribution of each sample should be its probability. To put it into context, we want to choose rays that are closer to the surface normal more often than rays on the equator.

This sampling strategy is called cosine weighted hemisphere sampling and is very easy to implement, requiring only a minor adjustment to the sampling function and evaluation

\begin{align}
z &= \sqrt{\xi_1} \\
\phi &= 2 \pi \xi_2 \\
x &= \sqrt{1 - z^{2}} \cos\phi \\
y &= \sqrt{1 - z^{2}} \sin\phi \\
PDF(z) &= \frac{z}{\pi} \\
\frac{\frac{\text{albedo}}{\pi}(\omega_i \cdot \mathbf{n})}{\frac{\omega_i \cdot \mathbf{n}}{\pi}} &= \frac{\text{albedo}}{\pi}(\omega_i \cdot \mathbf{n})\frac{\pi}{\omega_i \cdot \mathbf{n}} = \text{albedo}
\end{align}

The derivation for the PDF and the square root in $z$ are outside of the scope of this book, but you can find them [here](https://alexanderameye.github.io/notes/sampling-the-hemisphere).

In addition to implementing the above, we're also going to calculate the PDF and weight of the sample in our sample function, as we already know the dot product term

~~~ lua
local function SampleHemisphere(result, r1, r2)
	~~~ lua delete
	local sinTheta = math.sqrt(1 - r1 * r1)
	~~~ lua highlight
	local z = math.sqrt(r1)
	local sinTheta = math.sqrt(1 - r1)
	~~~ lua
	local phi = 2 * math.pi * r2

	
	~~~ lua delete
	return (
		sinTheta * math.cos(phi) * result:Tangent() +
		sinTheta * math.sin(phi) * result:Binormal() +
		r1                       * result:Normal()
	)
	~~~ lua highlight
	return {
		scattered = (
			sinTheta * math.cos(phi) * result:Tangent() +
			sinTheta * math.sin(phi) * result:Binormal() +
			z                        * result:Normal()
		),
		weight    = result:Albedo(),
		pdf       = z / math.pi
	}
	~~~ lua
end
~~~

And then in `TracePixel`

~~~ lua
			...

			for depth = 1, MAX_DEPTH do
				local sample = SampleHemisphere(result, sampler:GetFloat2D())
				~~~ lua delete
				throughput = throughput * 2 * result:Albedo() * sample:Dot(result:Normal())
				~~~ lua highlight
				throughput = throughput * sample.weight
				~~~ lua

				local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())

				~~~ lua delete
				result = accel:Traverse(origin, sample)
				~~~ lua highlight
				result = accel:Traverse(origin, sample.scattered)
				~~~ lua
				if not result then
					~~~ lua delete
					colour = colour + hdri:GetPixel(sample) * throughput
					~~~ lua highlight
					colour = colour + throughput * hdri:GetPixel(sample.scattered)
					~~~ lua
					break
				end
			end
~~~

![From top left to bottom right: 1spp uniform, 1spp importance, 512spp uniform, 512spp importance](images/importance-sampling.png)

They both converge to the same solution as they're both unbiased, however importance sampling converges faster.

Next Event Estimation
---------------------

_[Source code for this section](source-code/3.4.lua)_

While importance sampling only makes a big improvement to specular surfaces, next event estimation is the opposite.

Despite the seemingly complicated name, next event estimation is just an application of the direct light sampling we did when ray tracing to path tracing.

We'll first rename our `SampleHemisphere` function to the more accurate `SampleLambert`, as well as adding two new functions: `EvalLambert` and `EvalLambertPDF` (which we wont be using just yet)

~~~ lua delete
local function SampleHemisphere(result, r1, r2)
~~~ lua highlight
local function SampleLambert(result, r1, r2)
~~~ lua
	local z = math.sqrt(r1)
	local sinTheta = math.sqrt(1 - r1)
	local phi = 2 * math.pi * r2

	return {
		scattered = (
			sinTheta * math.cos(phi) * result:Tangent() +
			sinTheta * math.sin(phi) * result:Binormal() +
			z                        * result:Normal()
		),
		weight    = result:Albedo(),
		pdf       = z / math.pi
	}
end


~~~ lua highlight
local function EvalLambert(result, scattered)
	local cosTheta = scattered:Dot(result:Normal())
	if cosTheta <= 0 then return Vector(0, 0, 0) end

	return result:Albedo() / math.pi * cosTheta
end

local function EvalLambertPDF(result, scattered)
	local cosTheta = scattered:Dot(result:Normal())
	if cosTheta <= 0 then return 0 end

	return cosTheta / math.pi
end
~~~

Then we'll add a slight modification of our diffuse lighting from the ray tracer

~~~ lua
			for depth = 1, MAX_DEPTH do
				~~~ lua delete
				local sample = SampleHemisphere(result, sampler:GetFloat2D())
				throughput = throughput * sample.weight
				~~~ lua highlight
				local sample = SampleLambert(result, sampler:GetFloat2D())
				~~~ lua

				local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())


				~~~ lua highlight
				local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
				if envValid then
					local shadowRay = accel:Traverse(origin, envDir)
					if not shadowRay then
						colour = colour + throughput * EvalLambert(result, envDir) * envCol / envPdf
					end
				end

				throughput = throughput * sample.weight
				~~~ lua

				result = accel:Traverse(origin, sample.scattered)
				if not result then
					colour = colour + throughput * hdri:GetPixel(sample.scattered)
					break
				end
			end
~~~

![Left: importance sampling, right: importance sampling + next event estimation](images/next-event-estimation.png)

Multiple Importance Sampling
----------------------------

_[Source code for this section](source-code/3.5.lua)_

So importance sampling is good for smooth specular surfaces, and next event estimation is good for rough diffuse, surely there's a way to combine both strategies into one universally low variance integrator?

There is indeed, multiple importance sampling is a technique to combine any number of importance sampling strategies (hence the name) into a single low variance integrator, by weighting the contribution of each strategy by its probability relative to the other strategies. It's basically importance sampling importance samplers.

![Credit: https://graphics.stanford.edu/courses/cs348b-02/lectures/multipleimportance/walk002.html](https://graphics.stanford.edu/courses/cs348b-02/lectures/multipleimportance/slide002.png)

While this may look like another complex maths heavy section, the hardest part about MIS is calculating the PDFs of each importance sampling strategy, which is already done.

A detailed explanation of multiple importance sampling as well as many other heuristics can be found in [this Stanford research paper](https://graphics.stanford.edu/courses/cs348b-03/papers/veach-chapter9.pdf). For the sake of simplicity we will ignore the proof and instead assume that Veach knows what he's doing.

We're using the power heuristic here with $\beta = 2$ as it performs about the best across all common use cases

\begin{align}
w_i &= \frac{p_i^\beta}{\sum_k p_k^\beta} \\
w_i &= \frac{p_i^{2}}{\sum_k p_k^{2}} \\
w_i &= \frac{p_i^{2}}{p_{\text{bsdf}}^{2} + p_{\text{direct}}^{2}}
\end{align}

~~~ lua
local function EvalLambertPDF(result, scattered)
	local cosTheta = scattered:Dot(result:Normal())
	if cosTheta <= 0 then return 0 end

	return cosTheta / math.pi
end


~~~ lua highlight
local function Power2Heuristic(p0, p1)
	local p02 = p0 * p0
	return p02 / (p02 + p1 * p1)
end
~~~ lua

...
~~~

We can then weight both our direct lighting and indirect lighting by the heuristic, swapping the probabilities for each

~~~ lua
		local validSamples = SAMPLES
		for sample = 1, SAMPLES do
			local result = camRay
			local throughput = Vector(1, 1, 1)

			for depth = 1, MAX_DEPTH do
				local sample = SampleLambert(result, sampler:GetFloat2D())

				local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())

				local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
				if envValid then
					local shadowRay = accel:Traverse(origin, envDir)
					if not shadowRay then
						~~~ lua delete
						colour = colour + throughput * EvalLambert(result, envDir) * envCol / envPdf
						~~~ lua highlight
						local misWeight = Power2Heuristic(envPdf, EvalLambertPDF(result, envDir))
						colour = colour + throughput * EvalLambert(result, envDir) * envCol / envPdf * misWeight
						~~~ lua
					end
				end

				throughput = throughput * sample.weight

				result = accel:Traverse(origin, sample.scattered)
				if not result then
					~~~ lua delete
					colour = colour + throughput * hdri:GetPixel(sample.scattered)
					~~~ lua highlight
					local misWeight = Power2Heuristic(sample.pdf, hdri:EvalPDF(sample.scattered))
					colour = colour + throughput * hdri:GetPixel(sample.scattered) * misWeight
					~~~ lua
					break
				end
			end
		end
~~~

![From top left to bottom right: 1spp next event estimation, 1spp multiple importance sampling, 512spp next event estimation, 512spp multiple importance sampling](images/multiple-importance-sampling.png)

You may notice that the next event estimation only renders above are slightly brighter than with MIS, this is because we were double counting light by just adding together. You could of course solve this by dividing each contribution by $2$, but that would technically be multiple importance sampling.

Russian Roulette and Texture Filtering
--------------------------------------

_[Source code for this section](source-code/3.6.lua)_

One last subtle improvement to render times we can make is randomly terminating paths when their contribution is too low, a technique called russian roulette. This allows us to set our depth high to avoid biasing, while not increasing render times as much

~~~ lua
			for depth = 1, MAX_DEPTH do
				local sample = SampleLambert(result, sampler:GetFloat2D())

				local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())

				local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
				if envValid then
					local shadowRay = accel:Traverse(origin, envDir)
					if not shadowRay then
						local misWeight = Power2Heuristic(envPdf, EvalLambertPDF(result, envDir))
						colour = colour + throughput * EvalLambert(result, envDir) * envCol / envPdf * misWeight
					end
				end

				throughput = throughput * sample.weight


				~~~ lua highlight
				local rrProb = math.max(throughput[1], throughput[2], throughput[3])
				if sampler:GetFloat() >= rrProb then break end
				throughput = throughput / rrProb
				~~~ lua

				result = accel:Traverse(origin, sample.scattered)
				if not result then
					local misWeight = Power2Heuristic(sample.pdf, hdri:EvalPDF(sample.scattered))
					colour = colour + throughput * hdri:GetPixel(sample.scattered) * misWeight
					break
				end
			end
~~~

We can also reduce aliasing from textures viewed from afar with texture filtering.

VisTrace provides a mechanism for texture filtering via ray cones. Propagating a ray cone throughout each bounce is quite complex, however adding it to the camera rays is easy. I wont go over the formula for calculating the initial ray cone angle as it's not directly related to ray tracing but you can find it in Ray Tracing Gems 2.

~~~ lua
...

local sensorWidthDivRes = sensorWidth / RESX
local sensorHeightDivRes = SENSOR_HEIGHT_MM / RESY


~~~ lua highlight
local camConeAngle = math.atan(SENSOR_HEIGHT_MM / FOCAL_LENGTH_MM / RESY)
~~~ lua

...
~~~

We then pass our cone angle into `accel:Traverse(...)`, and use an initial cone width of 0

~~~ lua
	...

	local camDir = Vector(FOCAL_LENGTH_MM, camX, camY)
	camDir:Rotate(camAng)
	camDir:Normalize()


	~~~ lua delete
	local camRay = accel:Traverse(camPos, camDir)
	~~~ lua highlight
	local camRay = accel:Traverse(camPos, camDir, nil, nil, 0, camConeAngle)
	~~~ lua
	if camRay and not camRay:HitSky() then
		...
~~~

You may find that the filtering blurs too much at grazing angles. This is because VTFParser currently only performs isotropic trilinear filtering and can be artificially improved by multiplying the cone angle.

VisTrace BSDF
-------------

Finally, lets swap out our simple Lambert BRDF with the VisTrace BSDF.

First of all we add a new default material parameter

~~~ lua
----------------
-- Parameters --
----------------
local RESX, RESY = 512, 512
local SAMPLES = 16
local MAX_DEPTH = 16

local FOCAL_LENGTH_MM = 20
local SENSOR_HEIGHT_MM = 35


~~~ lua highlight
local DEFAULT_MATERIAL = vistrace.CreateMaterial()
-- Configure the default material here
~~~ lua

...
~~~

Then replace the calls to our BRDF with the VisTrace BSDF (you can delete those functions now). We're also calling `GetBSDFMaterial` on valid entities, which is set using the BSDF Material toolgun

~~~ lua
			for depth = 1, MAX_DEPTH do
				~~~ lua highlight
				local mat = result:Entity():IsValid() and result:Entity():GetBSDFMaterial() or DEFAULT_MATERIAL
				~~~ lua


				~~~ lua delete
				local sample = SampleLambert(result, sampler:GetFloat2D())
				~~~ lua highlight
				local sample = result:SampleBSDF(sampler, mat)
				if not sample then
					if depth == 1 then validSamples = validSamples - 1 end
					break
				end
				~~~ lua

				local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())

				local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
				if envValid then
					local shadowRay = accel:Traverse(origin, envDir)
					if not shadowRay then
						~~~ lua delete
						local misWeight = Power2Heuristic(envPdf, EvalLambertPDF(result, envDir))
						colour = colour + throughput * EvalLambert(result, envDir) * envCol / envPdf * misWeight
						~~~ lua highlight
						local misWeight = Power2Heuristic(envPdf, result:EvalPDF(mat, envDir))
						colour = colour + throughput * result:EvalBSDF(mat, envDir) * envCol / envPdf * misWeight
						~~~ lua
					end
				end

				throughput = throughput * sample.weight

				local rrProb = math.max(throughput[1], throughput[2], throughput[3])
				if sampler:GetFloat() >= rrProb then break end
				throughput = throughput / rrProb

				result = accel:Traverse(origin, sample.scattered)
				if not result then
					local misWeight = Power2Heuristic(sample.pdf, hdri:EvalPDF(sample.scattered))
					colour = colour + throughput * hdri:GetPixel(sample.scattered) * misWeight
					break
				end
			end
~~~

![From left to right: rough dielectric, smooth dielectric, rough conductor](images/vistrace-bsdf.png)

You can see the effect of one of the bundled PBR materials on the hunter plate.

We have to do two more things to support all the lobes of the BSDF though: disable NEE/MIS on delta (single reflection/transmission direction) lobes and invert the normal passed to `CalcRayOrigin` on transmission as we did when ray tracing

~~~ lua
			for depth = 1, MAX_DEPTH do
				local mat = result:Entity():IsValid() and result:Entity():GetBSDFMaterial() or DEFAULT_MATERIAL

				local sample = result:SampleBSDF(sampler, mat)
				if not sample then
					if depth == 1 then validSamples = validSamples - 1 end
					break
				end


				~~~ lua delete
				local origin = vistrace.CalcRayOrigin(result:Pos(), result:GeometricNormal())
				~~~ lua highlight
				local delta = bit.band(LobeType.Delta, sample.lobe) ~= 0
				local reflection = bit.band(LobeType.Transmission, sample.lobe) == 0
				local origin = vistrace.CalcRayOrigin(
					result:Pos(),
					(result:FrontFacing() == reflection) and result:GeometricNormal() or -result:GeometricNormal()
				)
				~~~ lua


				~~~ lua highlight
				if not delta then
				~~~ lua
					local envValid, envDir, envCol, envPdf = hdri:Sample(sampler)
					if envValid then
						local shadowRay = accel:Traverse(origin, envDir)
						if not shadowRay or shadowRay:HitSky() then
							local misWeight = Power2Heuristic(envPdf, result:EvalPDF(mat, envDir))
							colour = colour + throughput * result:EvalBSDF(mat, envDir) * envCol / envPdf * misWeight
						end
					end
				~~~ lua highlight
				end
				~~~ lua

				throughput = throughput * sample.weight

				local rrProb = math.max(throughput[1], throughput[2], throughput[3])
				if sampler:GetFloat() >= rrProb then break end
				throughput = throughput / rrProb

				result = accel:Traverse(origin, sample.scattered)
				if not result or result:HitSky() then
					~~~ lua delete
					local misWeight = Power2Heuristic(sample.pdf, hdri:EvalPDF(sample.scattered))
					~~~ lua highlight
					local misWeight = delta and 1 or Power2Heuristic(sample.pdf, hdri:EvalPDF(sample.scattered))
					~~~ lua
					colour = colour + throughput * hdri:GetPixel(sample.scattered) * misWeight
					break
				end
			end
~~~

![From left to right: delta conductor, rough glass, delta thin](images/vistrace-bsdf-transmission-delta.png)

Conclusion
----------

You now have a capable path tracer powered by the VisTrace BSDF, and you should also know enough about the theory of path tracing to implement both your own physically based techniques and those found in graphics blog posts and research papers.

But you don't have to stop there, you can continue reading for a deep dive into BSDFs themselves, or try implementing as many of the following features as you can:
* Enable world tracing by passing true to `CreateAccel` and checking `result:HitSky()` in addition to whether the ray missed
* Add a material for water and use `result:HitWater()` to apply it. Hint: map water surfaces are actually two planes stacked on top of each other, so you'll need to invert the normal when the ray points upwards and call the global BSDF functions `vistrace.EvalBSDF`, `vistrace.EvalPDF`, and `vistrace.SampleBSDF`
* Implement analytical lights. These can be added exactly the same way as HDRI direct sampling with a few simple ones being
	* Point lights
	* Area lights
	* Spot lights
* Add volumetrics. You could instead implement only Beer's law for specular transmission to get some easy absorption

![BSDF Showcase](images/path-tracing-conclusion.png)

Shading
=======

What is a BSDF?
---------------

The VisTrace BSDF
-----------------------

Writing Your Own
----------------

The Microfacet Model
--------------------

Choosing F, D, and G
-------------------

Putting it All Together
-----------------------

Further Reading
===============

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
